{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iPrGcTX8c7E-",
        "outputId": "04ac33e9-5e75-4397-e127-1dc8d4c7ce57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Feb 27 05:19:25 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   43C    P0    28W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzWrFN_tGV-Y"
      },
      "source": [
        "# Set Up New Environment\n",
        "We will mount GDRIVE so that your snapshots are saved there.  You must also place your training images in GDRIVE."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5ib935pEf32"
      },
      "source": [
        "# First time processing dataset (won't be needed for second time if you save processed dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cToowRb5eav0"
      },
      "outputs": [],
      "source": [
        "2! mkdir -p ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dB1W8kESe166",
        "outputId": "39fafb8d-2700-45bf-c23e-2c6909aae5ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading intel-image-classification.zip to /content\n",
            " 96% 331M/346M [00:02<00:00, 160MB/s]\n",
            "100% 346M/346M [00:02<00:00, 159MB/s]\n"
          ]
        }
      ],
      "source": [
        "!kaggle datasets download -d puneet6060/intel-image-classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jmU3fv3fe2Cy"
      },
      "outputs": [],
      "source": [
        "!mkdir Landscape\n",
        "!unzip -n intel-image-classification.zip -d Landscape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uxs1j1bk_fwj",
        "outputId": "881254e6-b084-4250-e771-3f340150beed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Note: using Google CoLab\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    COLAB = True\n",
        "    print(\"Note: using Google CoLab\")\n",
        "except:\n",
        "    print(\"Note: not using Google CoLab\")\n",
        "    COLAB = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dtllettdfcOh",
        "outputId": "8cbfb97d-f556-44a9-86c2-77039332bfe7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cp: cannot stat '/content/Landscape': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!cp -r /content/Landscape /content/drive/MyDrive/Datasets/Kaggle/Landscape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X41Ll0WtYqB0"
      },
      "source": [
        "You must also install NVIDIA StyleGAN2 ADA PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uNqsi6VWAlWo",
        "outputId": "ac7ebfad-7e40-4846-d62c-9fbb8fdd056d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'stylegan2-ada-pytorch'...\n",
            "remote: Enumerating objects: 128, done.\u001b[K\n",
            "remote: Total 128 (delta 0), reused 0 (delta 0), pack-reused 128\u001b[K\n",
            "Receiving objects: 100% (128/128), 1.12 MiB | 15.30 MiB/s, done.\n",
            "Resolving deltas: 100% (57/57), done.\n",
            "Collecting ninja\n",
            "  Downloading ninja-1.10.2.3-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (108 kB)\n",
            "\u001b[K     |████████████████████████████████| 108 kB 5.1 MB/s \n",
            "\u001b[?25hInstalling collected packages: ninja\n",
            "Successfully installed ninja-1.10.2.3\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/NVlabs/stylegan2-ada-pytorch.git\n",
        "!pip install ninja"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LGdFfTSXBBr5",
        "outputId": "26b98b42-f240-4d4d-ed05-42e342dab41d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10000.jpg  12567.jpg  15376.jpg  17820.jpg  2423.jpg  5013.jpg\t737.jpg\n",
            "10001.jpg  12579.jpg  15384.jpg  17840.jpg  2428.jpg  5017.jpg\t7387.jpg\n",
            "10002.jpg  1257.jpg   1538.jpg\t 17842.jpg  2433.jpg  5018.jpg\t7393.jpg\n",
            "10008.jpg  12584.jpg  153.jpg\t 17847.jpg  2435.jpg  5029.jpg\t7400.jpg\n",
            "10023.jpg  12588.jpg  15408.jpg  17860.jpg  2448.jpg  5032.jpg\t7401.jpg\n",
            "10026.jpg  12595.jpg  1540.jpg\t 17872.jpg  2455.jpg  5035.jpg\t7412.jpg\n",
            "10027.jpg  12598.jpg  15416.jpg  17882.jpg  2458.jpg  5038.jpg\t7416.jpg\n",
            "10028.jpg  12599.jpg  1541.jpg\t 17885.jpg  245.jpg   5040.jpg\t7421.jpg\n",
            "10031.jpg  12600.jpg  15425.jpg  1788.jpg   2471.jpg  5042.jpg\t743.jpg\n",
            "10035.jpg  12613.jpg  15430.jpg  17910.jpg  2477.jpg  5043.jpg\t7442.jpg\n",
            "10044.jpg  12616.jpg  15434.jpg  17916.jpg  2481.jpg  5044.jpg\t7448.jpg\n",
            "10046.jpg  12627.jpg  15435.jpg  17919.jpg  2492.jpg  5065.jpg\t7464.jpg\n",
            "10049.jpg  1262.jpg   15436.jpg  17920.jpg  2505.jpg  5075.jpg\t7474.jpg\n",
            "10057.jpg  12638.jpg  15444.jpg  17922.jpg  2517.jpg  5082.jpg\t748.jpg\n",
            "10058.jpg  12639.jpg  15445.jpg  17934.jpg  2531.jpg  5084.jpg\t7506.jpg\n",
            "10067.jpg  12640.jpg  15448.jpg  17939.jpg  2539.jpg  5086.jpg\t7537.jpg\n",
            "1006.jpg   12651.jpg  15462.jpg  17941.jpg  2543.jpg  5093.jpg\t7539.jpg\n",
            "10075.jpg  12652.jpg  15470.jpg  1794.jpg   254.jpg   5094.jpg\t7551.jpg\n",
            "10081.jpg  12654.jpg  15473.jpg  17954.jpg  2569.jpg  5117.jpg\t7560.jpg\n",
            "10105.jpg  12655.jpg  15475.jpg  17964.jpg  2570.jpg  5119.jpg\t7565.jpg\n",
            "10107.jpg  12661.jpg  15493.jpg  17968.jpg  2571.jpg  5120.jpg\t7578.jpg\n",
            "10136.jpg  12663.jpg  15502.jpg  17972.jpg  2573.jpg  5122.jpg\t7581.jpg\n",
            "10139.jpg  12667.jpg  15503.jpg  17980.jpg  2578.jpg  5124.jpg\t7586.jpg\n",
            "10156.jpg  12673.jpg  15504.jpg  17996.jpg  2579.jpg  5132.jpg\t7647.jpg\n",
            "10162.jpg  12678.jpg  15505.jpg  17998.jpg  258.jpg   5136.jpg\t7652.jpg\n",
            "10163.jpg  12681.jpg  15510.jpg  1799.jpg   2590.jpg  5153.jpg\t7654.jpg\n",
            "10174.jpg  12690.jpg  15521.jpg  17.jpg     2592.jpg  5156.jpg\t7662.jpg\n",
            "10181.jpg  12696.jpg  15532.jpg  18000.jpg  2604.jpg  5163.jpg\t7672.jpg\n",
            "10193.jpg  12698.jpg  15551.jpg  18007.jpg  2605.jpg  5168.jpg\t7679.jpg\n",
            "1019.jpg   12701.jpg  15554.jpg  18020.jpg  2609.jpg  5171.jpg\t767.jpg\n",
            "10206.jpg  12702.jpg  15557.jpg  18046.jpg  2615.jpg  5172.jpg\t7681.jpg\n",
            "10214.jpg  12708.jpg  15559.jpg  18053.jpg  2618.jpg  518.jpg\t7693.jpg\n",
            "10221.jpg  12710.jpg  15562.jpg  18073.jpg  2625.jpg  5192.jpg\t7695.jpg\n",
            "10229.jpg  12728.jpg  15568.jpg  18075.jpg  2630.jpg  5195.jpg\t7698.jpg\n",
            "10250.jpg  12747.jpg  1558.jpg\t 18078.jpg  2635.jpg  519.jpg\t7700.jpg\n",
            "10264.jpg  12748.jpg  15598.jpg  1809.jpg   2641.jpg  5201.jpg\t7715.jpg\n",
            "10265.jpg  12750.jpg  1559.jpg\t 18102.jpg  2647.jpg  5212.jpg\t771.jpg\n",
            "10268.jpg  12760.jpg  155.jpg\t 18104.jpg  2660.jpg  5216.jpg\t7744.jpg\n",
            "10273.jpg  12769.jpg  15601.jpg  18106.jpg  2663.jpg  5224.jpg\t7745.jpg\n",
            "10280.jpg  12776.jpg  15606.jpg  18116.jpg  2672.jpg  5234.jpg\t7751.jpg\n",
            "1029.jpg   1278.jpg   15608.jpg  18117.jpg  2677.jpg  5244.jpg\t7763.jpg\n",
            "10300.jpg  12795.jpg  15636.jpg  1811.jpg   2680.jpg  5252.jpg\t7771.jpg\n",
            "10319.jpg  12798.jpg  15639.jpg  18122.jpg  2687.jpg  5257.jpg\t7780.jpg\n",
            "10323.jpg  12801.jpg  15654.jpg  18125.jpg  2702.jpg  525.jpg\t7787.jpg\n",
            "10324.jpg  12815.jpg  1567.jpg\t 18131.jpg  2706.jpg  5263.jpg\t7788.jpg\n",
            "10329.jpg  12829.jpg  15684.jpg  18132.jpg  2720.jpg  5264.jpg\t7813.jpg\n",
            "1032.jpg   12837.jpg  15687.jpg  18135.jpg  2721.jpg  5272.jpg\t7816.jpg\n",
            "10332.jpg  12846.jpg  15689.jpg  18139.jpg  2724.jpg  5276.jpg\t7819.jpg\n",
            "10334.jpg  12848.jpg  15694.jpg  1813.jpg   2726.jpg  5281.jpg\t7820.jpg\n",
            "10361.jpg  1284.jpg   15696.jpg  18149.jpg  2730.jpg  5287.jpg\t7823.jpg\n",
            "10380.jpg  12877.jpg  15702.jpg  18159.jpg  2748.jpg  5294.jpg\t7836.jpg\n",
            "10384.jpg  1287.jpg   15704.jpg  18163.jpg  2752.jpg  5295.jpg\t7841.jpg\n",
            "10386.jpg  12886.jpg  15705.jpg  18170.jpg  2753.jpg  5296.jpg\t7842.jpg\n",
            "10393.jpg  12889.jpg  15708.jpg  18177.jpg  2755.jpg  5298.jpg\t7845.jpg\n",
            "10398.jpg  12903.jpg  15709.jpg  18186.jpg  2762.jpg  5306.jpg\t7849.jpg\n",
            "10407.jpg  12904.jpg  15719.jpg  18192.jpg  2769.jpg  5309.jpg\t784.jpg\n",
            "1040.jpg   12906.jpg  1573.jpg\t 1819.jpg   276.jpg   5330.jpg\t7851.jpg\n",
            "10418.jpg  12907.jpg  15740.jpg  18202.jpg  2772.jpg  5334.jpg\t7865.jpg\n",
            "10438.jpg  12909.jpg  15748.jpg  18204.jpg  2773.jpg  5357.jpg\t7875.jpg\n",
            "10440.jpg  12912.jpg  15755.jpg  18206.jpg  2799.jpg  5371.jpg\t7881.jpg\n",
            "10449.jpg  12915.jpg  15770.jpg  18213.jpg  2803.jpg  5373.jpg\t7885.jpg\n",
            "10453.jpg  12923.jpg  15771.jpg  18215.jpg  280.jpg   537.jpg\t7908.jpg\n",
            "10454.jpg  12928.jpg  15783.jpg  18218.jpg  282.jpg   5392.jpg\t7909.jpg\n",
            "10456.jpg  12937.jpg  15806.jpg  18219.jpg  2835.jpg  5393.jpg\t790.jpg\n",
            "10460.jpg  12944.jpg  15812.jpg  18224.jpg  2853.jpg  5395.jpg\t7912.jpg\n",
            "10462.jpg  12950.jpg  15816.jpg  18226.jpg  2863.jpg  5396.jpg\t7922.jpg\n",
            "10474.jpg  12976.jpg  15823.jpg  18239.jpg  2873.jpg  5398.jpg\t7928.jpg\n",
            "10482.jpg  12986.jpg  15826.jpg  18256.jpg  2875.jpg  5399.jpg\t7942.jpg\n",
            "10483.jpg  13000.jpg  15828.jpg  1825.jpg   2879.jpg  5411.jpg\t7946.jpg\n",
            "10496.jpg  13013.jpg  1582.jpg\t 18265.jpg  288.jpg   5412.jpg\t7960.jpg\n",
            "10506.jpg  13019.jpg  15835.jpg  18269.jpg  2912.jpg  5414.jpg\t7973.jpg\n",
            "10507.jpg  13021.jpg  15843.jpg  1826.jpg   2915.jpg  5417.jpg\t7979.jpg\n",
            "10515.jpg  13030.jpg  15872.jpg  18270.jpg  2919.jpg  5418.jpg\t7984.jpg\n",
            "10517.jpg  13041.jpg  15876.jpg  18273.jpg  2929.jpg  5422.jpg\t7993.jpg\n",
            "10518.jpg  13067.jpg  15886.jpg  18275.jpg  2930.jpg  5431.jpg\t8009.jpg\n",
            "10521.jpg  13071.jpg  1588.jpg\t 18280.jpg  2932.jpg  5433.jpg\t800.jpg\n",
            "10537.jpg  13072.jpg  15899.jpg  18287.jpg  293.jpg   5436.jpg\t8025.jpg\n",
            "10541.jpg  13094.jpg  15915.jpg  1828.jpg   2943.jpg  543.jpg\t8043.jpg\n",
            "10544.jpg  1309.jpg   15931.jpg  18290.jpg  2968.jpg  5459.jpg\t8044.jpg\n",
            "1054.jpg   13105.jpg  15932.jpg  18303.jpg  2976.jpg  5471.jpg\t8045.jpg\n",
            "10552.jpg  13117.jpg  15936.jpg  18309.jpg  297.jpg   5485.jpg\t805.jpg\n",
            "10554.jpg  13125.jpg  15965.jpg  18320.jpg  2983.jpg  5487.jpg\t8070.jpg\n",
            "10555.jpg  13126.jpg  15970.jpg  18322.jpg  2984.jpg  5489.jpg\t8088.jpg\n",
            "10563.jpg  13127.jpg  15971.jpg  18343.jpg  2995.jpg  5513.jpg\t8092.jpg\n",
            "10564.jpg  13128.jpg  15977.jpg  18345.jpg  2998.jpg  5514.jpg\t8105.jpg\n",
            "10566.jpg  13129.jpg  15978.jpg  18354.jpg  3005.jpg  5515.jpg\t8109.jpg\n",
            "10574.jpg  13139.jpg  1597.jpg\t 18355.jpg  301.jpg   5521.jpg\t8118.jpg\n",
            "10575.jpg  13141.jpg  15987.jpg  18380.jpg  3024.jpg  5529.jpg\t8125.jpg\n",
            "10581.jpg  13146.jpg  15991.jpg  18385.jpg  3030.jpg  5532.jpg\t8126.jpg\n",
            "10594.jpg  1314.jpg   15998.jpg  18398.jpg  3039.jpg  5536.jpg\t8136.jpg\n",
            "10599.jpg  13172.jpg  15999.jpg  18418.jpg  3044.jpg  5538.jpg\t8146.jpg\n",
            "10603.jpg  13173.jpg  16008.jpg  18432.jpg  3047.jpg  5540.jpg\t8148.jpg\n",
            "10609.jpg  13177.jpg  16009.jpg  18442.jpg  3050.jpg  5542.jpg\t8161.jpg\n",
            "10611.jpg  13180.jpg  1600.jpg\t 18454.jpg  305.jpg   554.jpg\t8163.jpg\n",
            "10623.jpg  13200.jpg  16024.jpg  1845.jpg   3070.jpg  5554.jpg\t8169.jpg\n",
            "10636.jpg  13203.jpg  16026.jpg  18474.jpg  3081.jpg  5556.jpg\t816.jpg\n",
            "10647.jpg  13219.jpg  1602.jpg\t 18486.jpg  3087.jpg  5561.jpg\t8195.jpg\n",
            "10659.jpg  13223.jpg  16034.jpg  18489.jpg  3088.jpg  5569.jpg\t8199.jpg\n",
            "1065.jpg   13229.jpg  16037.jpg  18498.jpg  3090.jpg  5576.jpg\t8200.jpg\n",
            "10666.jpg  13232.jpg  16041.jpg  18508.jpg  3094.jpg  5578.jpg\t820.jpg\n",
            "10667.jpg  13240.jpg  16046.jpg  18535.jpg  3104.jpg  5611.jpg\t8211.jpg\n",
            "1067.jpg   13245.jpg  16050.jpg  18539.jpg  3111.jpg  5615.jpg\t8213.jpg\n",
            "10692.jpg  13248.jpg  16074.jpg  18555.jpg  3114.jpg  5616.jpg\t821.jpg\n",
            "1069.jpg   13250.jpg  1607.jpg\t 18556.jpg  3124.jpg  5626.jpg\t8222.jpg\n",
            "10714.jpg  13254.jpg  16081.jpg  18561.jpg  3125.jpg  5627.jpg\t8223.jpg\n",
            "10716.jpg  13277.jpg  16087.jpg  18569.jpg  3138.jpg  5641.jpg\t822.jpg\n",
            "10727.jpg  1327.jpg   16088.jpg  18575.jpg  313.jpg   5655.jpg\t8235.jpg\n",
            "10738.jpg  13281.jpg  1608.jpg\t 18578.jpg  3150.jpg  5676.jpg\t823.jpg\n",
            "10743.jpg  13286.jpg  16097.jpg  1860.jpg   3157.jpg  5680.jpg\t8242.jpg\n",
            "10770.jpg  13313.jpg  1609.jpg\t 18615.jpg  3162.jpg  5692.jpg\t8245.jpg\n",
            "10783.jpg  13319.jpg  16104.jpg  18628.jpg  3163.jpg  5697.jpg\t8250.jpg\n",
            "10787.jpg  13324.jpg  16114.jpg  18630.jpg  3173.jpg  5698.jpg\t8252.jpg\n",
            "10791.jpg  13327.jpg  16126.jpg  18632.jpg  3178.jpg  5707.jpg\t8260.jpg\n",
            "10794.jpg  13339.jpg  16128.jpg  18637.jpg  3180.jpg  5719.jpg\t8266.jpg\n",
            "10800.jpg  13343.jpg  1612.jpg\t 18645.jpg  3188.jpg  571.jpg\t827.jpg\n",
            "10810.jpg  13348.jpg  16144.jpg  18646.jpg  3190.jpg  5720.jpg\t8291.jpg\n",
            "10811.jpg  13349.jpg  16150.jpg  18648.jpg  3210.jpg  5722.jpg\t8296.jpg\n",
            "10815.jpg  13355.jpg  16162.jpg  18654.jpg  3224.jpg  5730.jpg\t8297.jpg\n",
            "10821.jpg  1336.jpg   16164.jpg  18656.jpg  3227.jpg  574.jpg\t8308.jpg\n",
            "10824.jpg  13376.jpg  16168.jpg  18659.jpg  3232.jpg  5752.jpg\t8327.jpg\n",
            "10827.jpg  13393.jpg  16173.jpg  18663.jpg  3234.jpg  5753.jpg\t8331.jpg\n",
            "10837.jpg  13397.jpg  16179.jpg  18697.jpg  3236.jpg  5755.jpg\t8335.jpg\n",
            "10850.jpg  13405.jpg  16180.jpg  18699.jpg  3239.jpg  5764.jpg\t8343.jpg\n",
            "10857.jpg  1340.jpg   16203.jpg  1869.jpg   323.jpg   5774.jpg\t8349.jpg\n",
            "10859.jpg  13411.jpg  16207.jpg  18701.jpg  3241.jpg  5779.jpg\t8355.jpg\n",
            "10862.jpg  13419.jpg  1621.jpg\t 18726.jpg  3256.jpg  5790.jpg\t8356.jpg\n",
            "10863.jpg  13445.jpg  16222.jpg  18737.jpg  3269.jpg  5795.jpg\t8365.jpg\n",
            "10876.jpg  1344.jpg   16225.jpg  18740.jpg  3275.jpg  5799.jpg\t8373.jpg\n",
            "1087.jpg   13464.jpg  16227.jpg  18744.jpg  3280.jpg  5811.jpg\t8377.jpg\n",
            "10891.jpg  13495.jpg  16231.jpg  18757.jpg  3284.jpg  5812.jpg\t8386.jpg\n",
            "10895.jpg  13497.jpg  16236.jpg  1875.jpg   3291.jpg  5828.jpg\t8393.jpg\n",
            "10922.jpg  13505.jpg  16237.jpg  18764.jpg  3293.jpg  5832.jpg\t8404.jpg\n",
            "10932.jpg  1350.jpg   16244.jpg  18765.jpg  32.jpg    5836.jpg\t8409.jpg\n",
            "10965.jpg  13522.jpg  16245.jpg  18784.jpg  3302.jpg  5855.jpg\t8420.jpg\n",
            "10975.jpg  1353.jpg   16246.jpg  18789.jpg  3305.jpg  5869.jpg\t8433.jpg\n",
            "10977.jpg  13543.jpg  16248.jpg  18792.jpg  3306.jpg  5873.jpg\t8436.jpg\n",
            "11023.jpg  13546.jpg  16252.jpg  18796.jpg  3308.jpg  5877.jpg\t8438.jpg\n",
            "11032.jpg  13549.jpg  16257.jpg  18798.jpg  3313.jpg  5878.jpg\t8441.jpg\n",
            "11034.jpg  13553.jpg  16278.jpg  18805.jpg  331.jpg   5885.jpg\t8445.jpg\n",
            "11044.jpg  13559.jpg  16281.jpg  18823.jpg  3337.jpg  5886.jpg\t8450.jpg\n",
            "11049.jpg  13563.jpg  16283.jpg  18826.jpg  3341.jpg  5890.jpg\t8455.jpg\n",
            "11057.jpg  13577.jpg  16286.jpg  18837.jpg  3342.jpg  5895.jpg\t8466.jpg\n",
            "11063.jpg  1357.jpg   1628.jpg\t 18839.jpg  3358.jpg  58.jpg\t8470.jpg\n",
            "11066.jpg  13611.jpg  16290.jpg  1883.jpg   3367.jpg  5907.jpg\t8472.jpg\n",
            "11069.jpg  13613.jpg  16295.jpg  18840.jpg  3373.jpg  5915.jpg\t8478.jpg\n",
            "11075.jpg  13618.jpg  16309.jpg  18842.jpg  3385.jpg  5917.jpg\t8479.jpg\n",
            "11080.jpg  13627.jpg  16312.jpg  1887.jpg   3388.jpg  5921.jpg\t8484.jpg\n",
            "11090.jpg  1362.jpg   16322.jpg  18884.jpg  3399.jpg  5924.jpg\t8485.jpg\n",
            "11099.jpg  13648.jpg  16341.jpg  18894.jpg  3401.jpg  5925.jpg\t8486.jpg\n",
            "11100.jpg  13654.jpg  16350.jpg  18895.jpg  3423.jpg  5926.jpg\t8488.jpg\n",
            "11111.jpg  13656.jpg  16354.jpg  18901.jpg  3424.jpg  5928.jpg\t848.jpg\n",
            "11114.jpg  13658.jpg  16356.jpg  18912.jpg  3425.jpg  5937.jpg\t8498.jpg\n",
            "11118.jpg  13668.jpg  16358.jpg  18923.jpg  3437.jpg  5939.jpg\t8500.jpg\n",
            "11124.jpg  13673.jpg  16359.jpg  18930.jpg  3439.jpg  5940.jpg\t8532.jpg\n",
            "11125.jpg  13705.jpg  16371.jpg  18931.jpg  3442.jpg  5943.jpg\t8536.jpg\n",
            "11127.jpg  13711.jpg  16374.jpg  18933.jpg  3446.jpg  5960.jpg\t8549.jpg\n",
            "11132.jpg  13720.jpg  16375.jpg  18936.jpg  3460.jpg  5963.jpg\t8561.jpg\n",
            "11136.jpg  13730.jpg  16376.jpg  18946.jpg  3465.jpg  5984.jpg\t8564.jpg\n",
            "11148.jpg  13731.jpg  1637.jpg\t 18954.jpg  3476.jpg  6000.jpg\t8571.jpg\n",
            "11164.jpg  13737.jpg  16381.jpg  18960.jpg  3487.jpg  6003.jpg\t8573.jpg\n",
            "11171.jpg  13761.jpg  16383.jpg  18967.jpg  3490.jpg  6006.jpg\t857.jpg\n",
            "11180.jpg  13781.jpg  16388.jpg  1896.jpg   34.jpg    6012.jpg\t8587.jpg\n",
            "11189.jpg  13788.jpg  16402.jpg  18972.jpg  3509.jpg  6018.jpg\t8594.jpg\n",
            "11215.jpg  13792.jpg  16409.jpg  18974.jpg  3527.jpg  6022.jpg\t85.jpg\n",
            "11217.jpg  1380.jpg   16417.jpg  18982.jpg  3535.jpg  6032.jpg\t8607.jpg\n",
            "11218.jpg  13818.jpg  16421.jpg  1898.jpg   3548.jpg  6034.jpg\t860.jpg\n",
            "11220.jpg  13825.jpg  1642.jpg\t 1899.jpg   3554.jpg  6036.jpg\t8615.jpg\n",
            "11223.jpg  13829.jpg  16435.jpg  19004.jpg  3568.jpg  6057.jpg\t8617.jpg\n",
            "11231.jpg  13836.jpg  16438.jpg  19007.jpg  3569.jpg  6067.jpg\t8628.jpg\n",
            "11232.jpg  13849.jpg  16439.jpg  19013.jpg  3575.jpg  6070.jpg\t8629.jpg\n",
            "11254.jpg  13855.jpg  16458.jpg  19015.jpg  3587.jpg  6079.jpg\t8635.jpg\n",
            "11265.jpg  13859.jpg  16459.jpg  19024.jpg  3592.jpg  6083.jpg\t8640.jpg\n",
            "11276.jpg  13864.jpg  1645.jpg\t 19035.jpg  3593.jpg  6088.jpg\t8655.jpg\n",
            "1127.jpg   13868.jpg  16460.jpg  19038.jpg  3598.jpg  6100.jpg\t8656.jpg\n",
            "11282.jpg  13885.jpg  16469.jpg  1903.jpg   3599.jpg  6101.jpg\t866.jpg\n",
            "11284.jpg  1389.jpg   1646.jpg\t 19041.jpg  3603.jpg  610.jpg\t8670.jpg\n",
            "11291.jpg  13909.jpg  16473.jpg  19051.jpg  3606.jpg  6114.jpg\t8676.jpg\n",
            "1129.jpg   13928.jpg  16479.jpg  19066.jpg  3608.jpg  6115.jpg\t8679.jpg\n",
            "11300.jpg  13930.jpg  16503.jpg  19071.jpg  3618.jpg  6126.jpg\t8684.jpg\n",
            "11310.jpg  13934.jpg  16506.jpg  1907.jpg   3620.jpg  6129.jpg\t8685.jpg\n",
            "11319.jpg  13938.jpg  16510.jpg  1909.jpg   3629.jpg  6147.jpg\t8692.jpg\n",
            "11326.jpg  13958.jpg  16515.jpg  19114.jpg  3630.jpg  6151.jpg\t8695.jpg\n",
            "1133.jpg   13959.jpg  16518.jpg  19120.jpg  3641.jpg  6156.jpg\t8703.jpg\n",
            "11340.jpg  13962.jpg  1651.jpg\t 19136.jpg  364.jpg   6157.jpg\t8724.jpg\n",
            "11346.jpg  1396.jpg   16529.jpg  19137.jpg  3650.jpg  6159.jpg\t8725.jpg\n",
            "11355.jpg  13997.jpg  16531.jpg  19162.jpg  3657.jpg  6167.jpg\t8734.jpg\n",
            "11358.jpg  14024.jpg  16533.jpg  19173.jpg  3658.jpg  6172.jpg\t8742.jpg\n",
            "11362.jpg  14025.jpg  16560.jpg  1917.jpg   3662.jpg  6210.jpg\t8744.jpg\n",
            "11367.jpg  14029.jpg  16570.jpg  19186.jpg  366.jpg   6215.jpg\t8749.jpg\n",
            "11383.jpg  14036.jpg  16582.jpg  19193.jpg  3676.jpg  6222.jpg\t8757.jpg\n",
            "11387.jpg  14039.jpg  16586.jpg  1919.jpg   3685.jpg  6239.jpg\t8762.jpg\n",
            "11393.jpg  14043.jpg  1658.jpg\t 19207.jpg  3689.jpg  6243.jpg\t8766.jpg\n",
            "11396.jpg  14060.jpg  16590.jpg  19214.jpg  3692.jpg  6254.jpg\t8768.jpg\n",
            "11402.jpg  14080.jpg  16592.jpg  19237.jpg  369.jpg   6256.jpg\t8771.jpg\n",
            "11409.jpg  1408.jpg   16598.jpg  19238.jpg  3701.jpg  6263.jpg\t8773.jpg\n",
            "1140.jpg   14095.jpg  16599.jpg  19240.jpg  3712.jpg  6267.jpg\t8780.jpg\n",
            "11436.jpg  14096.jpg  16604.jpg  19250.jpg  3719.jpg  6268.jpg\t8782.jpg\n",
            "11439.jpg  14105.jpg  1661.jpg\t 19251.jpg  3733.jpg  6272.jpg\t8791.jpg\n",
            "11441.jpg  14111.jpg  16627.jpg  19259.jpg  3742.jpg  6273.jpg\t8798.jpg\n",
            "11450.jpg  14117.jpg  16629.jpg  1925.jpg   3748.jpg  6276.jpg\t8803.jpg\n",
            "11457.jpg  14118.jpg  16636.jpg  19266.jpg  3756.jpg  6278.jpg\t8811.jpg\n",
            "11460.jpg  1412.jpg   16640.jpg  19268.jpg  3765.jpg  6295.jpg\t8816.jpg\n",
            "11464.jpg  14135.jpg  16644.jpg  19270.jpg  3779.jpg  6311.jpg\t8831.jpg\n",
            "11467.jpg  14140.jpg  16645.jpg  19276.jpg  377.jpg   6314.jpg\t883.jpg\n",
            "1147.jpg   14142.jpg  16655.jpg  19281.jpg  3786.jpg  6323.jpg\t8845.jpg\n",
            "11480.jpg  14147.jpg  16659.jpg  19290.jpg  3787.jpg  6327.jpg\t884.jpg\n",
            "11487.jpg  14154.jpg  16664.jpg  19295.jpg  378.jpg   6330.jpg\t8855.jpg\n",
            "11490.jpg  14175.jpg  16669.jpg  19296.jpg  3791.jpg  6358.jpg\t8862.jpg\n",
            "11491.jpg  14179.jpg  1666.jpg\t 19298.jpg  3794.jpg  6363.jpg\t8863.jpg\n",
            "11497.jpg  14190.jpg  16671.jpg  19303.jpg  3806.jpg  6367.jpg\t8881.jpg\n",
            "1149.jpg   14192.jpg  16673.jpg  19307.jpg  380.jpg   6385.jpg\t8883.jpg\n",
            "11504.jpg  14196.jpg  16674.jpg  19312.jpg  3826.jpg  6391.jpg\t8895.jpg\n",
            "11508.jpg  14198.jpg  16675.jpg  19318.jpg  3828.jpg  6398.jpg\t8902.jpg\n",
            "11512.jpg  14203.jpg  16677.jpg  19329.jpg  3849.jpg  6406.jpg\t8924.jpg\n",
            "11515.jpg  14207.jpg  16683.jpg  1933.jpg   3850.jpg  6407.jpg\t8932.jpg\n",
            "11516.jpg  1420.jpg   16696.jpg  19343.jpg  3857.jpg  6411.jpg\t8934.jpg\n",
            "1151.jpg   14221.jpg  16701.jpg  19354.jpg  3858.jpg  6440.jpg\t8937.jpg\n",
            "11524.jpg  14230.jpg  16709.jpg  19358.jpg  3860.jpg  6445.jpg\t8940.jpg\n",
            "11528.jpg  14239.jpg  16745.jpg  1936.jpg   3866.jpg  6452.jpg\t8960.jpg\n",
            "11544.jpg  14243.jpg  1674.jpg\t 19387.jpg  3872.jpg  6455.jpg\t8961.jpg\n",
            "11554.jpg  14245.jpg  16756.jpg  19388.jpg  3890.jpg  6465.jpg\t8991.jpg\n",
            "1155.jpg   14255.jpg  16763.jpg  19391.jpg  3896.jpg  6467.jpg\t899.jpg\n",
            "11562.jpg  1426.jpg   16764.jpg  19399.jpg  3898.jpg  6470.jpg\t9012.jpg\n",
            "11597.jpg  14270.jpg  16765.jpg  19400.jpg  3909.jpg  6471.jpg\t9023.jpg\n",
            "1160.jpg   14282.jpg  16770.jpg  19430.jpg  3915.jpg  6472.jpg\t9029.jpg\n",
            "11626.jpg  14283.jpg  16798.jpg  19434.jpg  3934.jpg  6482.jpg\t9045.jpg\n",
            "11637.jpg  14288.jpg  16801.jpg  19437.jpg  3936.jpg  6494.jpg\t9046.jpg\n",
            "1165.jpg   14299.jpg  16805.jpg  19449.jpg  3940.jpg  6495.jpg\t9048.jpg\n",
            "11669.jpg  14311.jpg  16809.jpg  19451.jpg  3941.jpg  6498.jpg\t9050.jpg\n",
            "11672.jpg  14314.jpg  16810.jpg  19468.jpg  3943.jpg  6510.jpg\t9064.jpg\n",
            "11677.jpg  14319.jpg  16826.jpg  19487.jpg  395.jpg   6512.jpg\t9065.jpg\n",
            "11680.jpg  14323.jpg  16831.jpg  19493.jpg  3963.jpg  6518.jpg\t9068.jpg\n",
            "11690.jpg  1432.jpg   16833.jpg  19495.jpg  3967.jpg  6540.jpg\t9070.jpg\n",
            "11691.jpg  14334.jpg  16842.jpg  19499.jpg  3969.jpg  6545.jpg\t9071.jpg\n",
            "11712.jpg  14340.jpg  16857.jpg  194.jpg    3973.jpg  6548.jpg\t9073.jpg\n",
            "11732.jpg  14341.jpg  1685.jpg\t 19519.jpg  3974.jpg  6549.jpg\t9083.jpg\n",
            "11748.jpg  14363.jpg  16863.jpg  19529.jpg  3977.jpg  6560.jpg\t9088.jpg\n",
            "11750.jpg  14373.jpg  16869.jpg  19532.jpg  3978.jpg  6589.jpg\t9105.jpg\n",
            "11762.jpg  14379.jpg  16884.jpg  19538.jpg  397.jpg   6592.jpg\t9120.jpg\n",
            "11779.jpg  14400.jpg  16901.jpg  19548.jpg  3990.jpg  659.jpg\t9133.jpg\n",
            "11790.jpg  14403.jpg  16904.jpg  19554.jpg  3994.jpg  6600.jpg\t9146.jpg\n",
            "11810.jpg  14412.jpg  16925.jpg  1956.jpg   4009.jpg  6601.jpg\t9147.jpg\n",
            "11816.jpg  1441.jpg   16927.jpg  19573.jpg  4010.jpg  6602.jpg\t914.jpg\n",
            "1181.jpg   14437.jpg  16960.jpg  19589.jpg  4026.jpg  6606.jpg\t9153.jpg\n",
            "11828.jpg  14443.jpg  16962.jpg  19600.jpg  4028.jpg  6607.jpg\t9155.jpg\n",
            "1183.jpg   14446.jpg  16966.jpg  19601.jpg  402.jpg   6627.jpg\t9174.jpg\n",
            "11842.jpg  14462.jpg  16971.jpg  1960.jpg   4062.jpg  6632.jpg\t9179.jpg\n",
            "1184.jpg   1446.jpg   16972.jpg  19616.jpg  4065.jpg  6633.jpg\t9186.jpg\n",
            "11856.jpg  14472.jpg  16982.jpg  19617.jpg  4102.jpg  6637.jpg\t9190.jpg\n",
            "11858.jpg  14483.jpg  16983.jpg  1961.jpg   4117.jpg  6638.jpg\t9195.jpg\n",
            "11864.jpg  14499.jpg  16989.jpg  19621.jpg  4120.jpg  6645.jpg\t9209.jpg\n",
            "11865.jpg  14505.jpg  1698.jpg\t 19625.jpg  4145.jpg  6656.jpg\t9213.jpg\n",
            "11871.jpg  14518.jpg  16991.jpg  1962.jpg   4148.jpg  6664.jpg\t9228.jpg\n",
            "11873.jpg  14523.jpg  1699.jpg\t 19642.jpg  414.jpg   6667.jpg\t9229.jpg\n",
            "11876.jpg  14529.jpg  16.jpg\t 19647.jpg  4151.jpg  6670.jpg\t9232.jpg\n",
            "11877.jpg  1454.jpg   17004.jpg  19677.jpg  4154.jpg  6671.jpg\t9239.jpg\n",
            "11887.jpg  14560.jpg  17005.jpg  19691.jpg  4155.jpg  6676.jpg\t9252.jpg\n",
            "11894.jpg  14564.jpg  17006.jpg  19692.jpg  415.jpg   6678.jpg\t9267.jpg\n",
            "11898.jpg  14575.jpg  1700.jpg\t 19709.jpg  4162.jpg  6695.jpg\t9273.jpg\n",
            "118.jpg    14579.jpg  17015.jpg  19714.jpg  4163.jpg  6697.jpg\t9276.jpg\n",
            "11927.jpg  14582.jpg  17019.jpg  19719.jpg  4173.jpg  6698.jpg\t9277.jpg\n",
            "11932.jpg  14592.jpg  17021.jpg  19749.jpg  4174.jpg  669.jpg\t9285.jpg\n",
            "11933.jpg  14597.jpg  17026.jpg  19756.jpg  4189.jpg  6709.jpg\t9286.jpg\n",
            "11940.jpg  14600.jpg  1702.jpg\t 19760.jpg  4192.jpg  6711.jpg\t9298.jpg\n",
            "11955.jpg  14602.jpg  17031.jpg  19762.jpg  4208.jpg  6712.jpg\t9309.jpg\n",
            "11956.jpg  14605.jpg  17043.jpg  19769.jpg  4209.jpg  6726.jpg\t9311.jpg\n",
            "11964.jpg  1460.jpg   17049.jpg  19771.jpg  4215.jpg  6728.jpg\t9321.jpg\n",
            "11981.jpg  1461.jpg   17061.jpg  19781.jpg  4224.jpg  6735.jpg\t9324.jpg\n",
            "11982.jpg  14637.jpg  17066.jpg  19791.jpg  4228.jpg  675.jpg\t9325.jpg\n",
            "11989.jpg  14641.jpg  17069.jpg  19793.jpg  4234.jpg  6760.jpg\t932.jpg\n",
            "12007.jpg  14644.jpg  17091.jpg  19794.jpg  4260.jpg  6766.jpg\t9330.jpg\n",
            "12008.jpg  14646.jpg  17097.jpg  19813.jpg  4261.jpg  6770.jpg\t9340.jpg\n",
            "12014.jpg  14647.jpg  17102.jpg  19827.jpg  4263.jpg  6771.jpg\t9348.jpg\n",
            "12030.jpg  1464.jpg   1714.jpg\t 19840.jpg  4281.jpg  6782.jpg\t9358.jpg\n",
            "1203.jpg   14655.jpg  17173.jpg  19843.jpg  4283.jpg  6790.jpg\t9360.jpg\n",
            "12043.jpg  1465.jpg   17178.jpg  19857.jpg  4299.jpg  6809.jpg\t9365.jpg\n",
            "12047.jpg  14674.jpg  17184.jpg  19859.jpg  42.jpg    6813.jpg\t9387.jpg\n",
            "12066.jpg  14698.jpg  17204.jpg  19861.jpg  4303.jpg  6821.jpg\t9393.jpg\n",
            "12069.jpg  14703.jpg  17209.jpg  19872.jpg  4311.jpg  6827.jpg\t9399.jpg\n",
            "12081.jpg  14711.jpg  17212.jpg  19887.jpg  4316.jpg  683.jpg\t9409.jpg\n",
            "12084.jpg  14716.jpg  1721.jpg\t 19888.jpg  4320.jpg  6842.jpg\t941.jpg\n",
            "1209.jpg   14719.jpg  17220.jpg  19889.jpg  4330.jpg  684.jpg\t9423.jpg\n",
            "120.jpg    14725.jpg  17226.jpg  19897.jpg  4339.jpg  6852.jpg\t9430.jpg\n",
            "12105.jpg  14726.jpg  17232.jpg  19903.jpg  4344.jpg  6868.jpg\t9434.jpg\n",
            "12117.jpg  14736.jpg  17234.jpg  19912.jpg  4362.jpg  6869.jpg\t9444.jpg\n",
            "12118.jpg  1473.jpg   17240.jpg  19927.jpg  4378.jpg  686.jpg\t9445.jpg\n",
            "12138.jpg  14743.jpg  17242.jpg  19929.jpg  4380.jpg  6873.jpg\t9455.jpg\n",
            "12140.jpg  14764.jpg  17244.jpg  19935.jpg  4384.jpg  6877.jpg\t9460.jpg\n",
            "12142.jpg  14788.jpg  17248.jpg  19936.jpg  4387.jpg  6883.jpg\t9474.jpg\n",
            "12143.jpg  14794.jpg  17260.jpg  19938.jpg  4389.jpg  6888.jpg\t9493.jpg\n",
            "12145.jpg  14804.jpg  17277.jpg  19939.jpg  4390.jpg  6907.jpg\t9502.jpg\n",
            "12151.jpg  14807.jpg  17286.jpg  1993.jpg   439.jpg   6914.jpg\t9528.jpg\n",
            "12158.jpg  1480.jpg   17317.jpg  19953.jpg  4406.jpg  6915.jpg\t9531.jpg\n",
            "12159.jpg  14824.jpg  17335.jpg  19954.jpg  4414.jpg  6919.jpg\t9533.jpg\n",
            "12164.jpg  14827.jpg  17339.jpg  19956.jpg  4417.jpg  6926.jpg\t9534.jpg\n",
            "12167.jpg  14832.jpg  17344.jpg  19959.jpg  443.jpg   6932.jpg\t9536.jpg\n",
            "1216.jpg   14838.jpg  17348.jpg  19962.jpg  4442.jpg  6934.jpg\t9551.jpg\n",
            "12177.jpg  14840.jpg  17364.jpg  19964.jpg  4456.jpg  6944.jpg\t9553.jpg\n",
            "12179.jpg  14844.jpg  17370.jpg  1996.jpg   4470.jpg  6949.jpg\t9560.jpg\n",
            "12191.jpg  14849.jpg  17379.jpg  19981.jpg  4483.jpg  6969.jpg\t9561.jpg\n",
            "12195.jpg  14866.jpg  17380.jpg  19987.jpg  4497.jpg  6972.jpg\t9568.jpg\n",
            "12197.jpg  148.jpg    17384.jpg  19989.jpg  44.jpg    6986.jpg\t956.jpg\n",
            "12202.jpg  14907.jpg  17385.jpg  20008.jpg  4501.jpg  6987.jpg\t9588.jpg\n",
            "12218.jpg  14927.jpg  17387.jpg  20014.jpg  4508.jpg  6994.jpg\t959.jpg\n",
            "12220.jpg  14936.jpg  1738.jpg\t 20019.jpg  4513.jpg  7002.jpg\t95.jpg\n",
            "12222.jpg  14938.jpg  17390.jpg  20026.jpg  4516.jpg  7015.jpg\t9610.jpg\n",
            "12230.jpg  14940.jpg  17424.jpg  2003.jpg   4522.jpg  7018.jpg\t9633.jpg\n",
            "12237.jpg  14948.jpg  17429.jpg  2004.jpg   4539.jpg  7020.jpg\t9639.jpg\n",
            "12239.jpg  14950.jpg  17434.jpg  20052.jpg  4541.jpg  7021.jpg\t9642.jpg\n",
            "12252.jpg  14969.jpg  17435.jpg  2005.jpg   4554.jpg  7026.jpg\t9643.jpg\n",
            "12256.jpg  14985.jpg  1743.jpg\t 202.jpg    4579.jpg  7037.jpg\t9644.jpg\n",
            "12262.jpg  14986.jpg  17441.jpg  2053.jpg   457.jpg   7047.jpg\t964.jpg\n",
            "12282.jpg  14987.jpg  17466.jpg  2054.jpg   459.jpg   7049.jpg\t9653.jpg\n",
            "12283.jpg  14988.jpg  17496.jpg  2078.jpg   4601.jpg  7060.jpg\t9669.jpg\n",
            "1228.jpg   14997.jpg  17502.jpg  2079.jpg   4602.jpg  7061.jpg\t9681.jpg\n",
            "12296.jpg  14999.jpg  17504.jpg  2087.jpg   4608.jpg  7063.jpg\t9682.jpg\n",
            "12299.jpg  1500.jpg   17507.jpg  209.jpg    4624.jpg  7073.jpg\t9686.jpg\n",
            "122.jpg    15017.jpg  17511.jpg  2105.jpg   4638.jpg  7083.jpg\t9689.jpg\n",
            "12311.jpg  15029.jpg  17519.jpg  2110.jpg   4639.jpg  708.jpg\t9703.jpg\n",
            "12317.jpg  15034.jpg  17529.jpg  2120.jpg   4644.jpg  7103.jpg\t9719.jpg\n",
            "12320.jpg  15044.jpg  17530.jpg  2134.jpg   4672.jpg  7105.jpg\t9728.jpg\n",
            "12324.jpg  15056.jpg  17553.jpg  2140.jpg   4673.jpg  710.jpg\t972.jpg\n",
            "12337.jpg  15065.jpg  17558.jpg  2142.jpg   4676.jpg  7113.jpg\t9739.jpg\n",
            "12339.jpg  15069.jpg  1755.jpg\t 2147.jpg   4682.jpg  7121.jpg\t9748.jpg\n",
            "12344.jpg  1506.jpg   17577.jpg  2149.jpg   4695.jpg  7126.jpg\t9756.jpg\n",
            "12353.jpg  15094.jpg  17579.jpg  2150.jpg   4704.jpg  7139.jpg\t9763.jpg\n",
            "12368.jpg  15108.jpg  17588.jpg  2161.jpg   4727.jpg  7142.jpg\t976.jpg\n",
            "1237.jpg   15120.jpg  1758.jpg\t 2166.jpg   4755.jpg  7144.jpg\t9771.jpg\n",
            "12381.jpg  15125.jpg  17621.jpg  2168.jpg   4768.jpg  7152.jpg\t9776.jpg\n",
            "12383.jpg  1513.jpg   17622.jpg  2174.jpg   4771.jpg  7157.jpg\t9779.jpg\n",
            "12386.jpg  15145.jpg  17624.jpg  2175.jpg   4776.jpg  7165.jpg\t9793.jpg\n",
            "1238.jpg   15155.jpg  17643.jpg  2181.jpg   4803.jpg  7167.jpg\t9799.jpg\n",
            "12405.jpg  15160.jpg  17647.jpg  2183.jpg   480.jpg   7169.jpg\t9801.jpg\n",
            "12408.jpg  15166.jpg  17648.jpg  2196.jpg   4826.jpg  71.jpg\t9805.jpg\n",
            "12412.jpg  15168.jpg  17661.jpg  2197.jpg   4850.jpg  7204.jpg\t9806.jpg\n",
            "12433.jpg  15180.jpg  17663.jpg  2199.jpg   4851.jpg  7209.jpg\t9822.jpg\n",
            "12439.jpg  15189.jpg  17674.jpg  2201.jpg   4855.jpg  7226.jpg\t9825.jpg\n",
            "12445.jpg  15190.jpg  17690.jpg  2211.jpg   4856.jpg  7247.jpg\t9832.jpg\n",
            "12467.jpg  15204.jpg  17700.jpg  2218.jpg   4857.jpg  7248.jpg\t9835.jpg\n",
            "12472.jpg  15206.jpg  17705.jpg  2219.jpg   4859.jpg  7252.jpg\t9840.jpg\n",
            "12477.jpg  15226.jpg  17712.jpg  2220.jpg   4862.jpg  7255.jpg\t9851.jpg\n",
            "12483.jpg  15235.jpg  17714.jpg  2230.jpg   4864.jpg  7268.jpg\t9856.jpg\n",
            "12496.jpg  15254.jpg  17717.jpg  2241.jpg   4875.jpg  7278.jpg\t9860.jpg\n",
            "12504.jpg  15263.jpg  17726.jpg  2269.jpg   4886.jpg  7289.jpg\t9861.jpg\n",
            "12505.jpg  15269.jpg  17729.jpg  2281.jpg   4913.jpg  728.jpg\t9868.jpg\n",
            "12509.jpg  1526.jpg   17744.jpg  2293.jpg   4923.jpg  7298.jpg\t9869.jpg\n",
            "12515.jpg  15277.jpg  17747.jpg  2298.jpg   4924.jpg  729.jpg\t9884.jpg\n",
            "12518.jpg  15281.jpg  17755.jpg  2303.jpg   4927.jpg  7306.jpg\t9894.jpg\n",
            "12521.jpg  15287.jpg  17756.jpg  2311.jpg   4941.jpg  730.jpg\t9896.jpg\n",
            "12522.jpg  15296.jpg  17767.jpg  2320.jpg   4951.jpg  7326.jpg\t9901.jpg\n",
            "12526.jpg  15306.jpg  17775.jpg  232.jpg    4953.jpg  7332.jpg\t9907.jpg\n",
            "12528.jpg  15316.jpg  17786.jpg  234.jpg    4964.jpg  7346.jpg\t9924.jpg\n",
            "12529.jpg  15319.jpg  17787.jpg  2382.jpg   4970.jpg  7350.jpg\t9928.jpg\n",
            "1252.jpg   15321.jpg  17789.jpg  2386.jpg   4977.jpg  7351.jpg\t9941.jpg\n",
            "12534.jpg  15328.jpg  17797.jpg  2388.jpg   4980.jpg  7354.jpg\t9952.jpg\n",
            "12547.jpg  15354.jpg  17799.jpg  2390.jpg   4992.jpg  7355.jpg\t9957.jpg\n",
            "12549.jpg  15355.jpg  177.jpg\t 2413.jpg   5007.jpg  7359.jpg\t9980.jpg\n",
            "12558.jpg  15367.jpg  17809.jpg  2416.jpg   500.jpg   7360.jpg\t9990.jpg\n",
            "12559.jpg  15371.jpg  17810.jpg  2422.jpg   5012.jpg  7375.jpg\n"
          ]
        }
      ],
      "source": [
        "!ls /content/drive/MyDrive/Datasets/Kaggle/Landscape/seg_train/seg_train/mountain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXAgXh8uGo13"
      },
      "source": [
        "# Convert Your Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lYP2NlwHA6r9",
        "outputId": "46d81d59-f6fc-4b62-dfa6-0275a9ca9596"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100% 2512/2512 [17:30<00:00,  2.39it/s]\n"
          ]
        }
      ],
      "source": [
        "!python /content/stylegan2-ada-pytorch/dataset_tool.py --source /content/drive/MyDrive/Datasets/Kaggle/Landscape/seg_train/seg_train/mountain --dest /content/drive/MyDrive/Datasets/Kaggle/Landscape/seg_train/seg_train/High_res_Processed_mountain --width 1024 --height 1024"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i8Vsh8E5tY_p"
      },
      "outputs": [],
      "source": [
        "!rm -r /content/drive/MyDrive/Datasets/Kaggle/Landscape/experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otsNELpn8_2D"
      },
      "source": [
        "The following command can be used to clear out the newly created dataset.  If something goes wrong and you need to clean up your images and rerun the above command, you should delete your partially created dataset directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ctdqmU96BhB3"
      },
      "outputs": [],
      "source": [
        "#!rm -R /content/drive/MyDrive/data/gan/dataset/circuit/*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmSOo3HvGwgV"
      },
      "source": [
        "# Clean Up your Images\n",
        "\n",
        "It is important that all images have the same dimensions and color depth.  This code can identify images that have issues."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "6e2e966131b54a0c927d8ff512ba3205",
            "7cfd0845dc414799b7e041cdb540b87b",
            "bfe48e018c6a4ac898182163491c2a15",
            "15f2d69ea54e4f809b3eb8ba056055cb",
            "979d78ed5d3a4dc5a1e7fd00ed24bcea",
            "56d424255bf142bc97ffc9bfbb4b7f9a",
            "7556cf7ae5c744aaafb0dd64381decb8",
            "08f2898142b044479517f525d2c82454",
            "e88bba6ffb7342a9a35c67f2287f23d7",
            "ad7fb1133ad548d3a04797f3176043f3",
            "b5f59378237f41cc81044e4222ed3e68"
          ]
        },
        "id": "FLKBUUfXHJ0a",
        "outputId": "fa1d3a9b-acc9-4f28-fa71-bca976fbfe94"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6e2e966131b54a0c927d8ff512ba3205",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/2512 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from os import listdir\n",
        "from os.path import isfile, join, isdir\n",
        "import os\n",
        "from PIL import Image\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "IMAGE_PATH = '/content/drive/MyDrive/Landscape/seg_train/seg_train/High_res_Processed_mountain'\n",
        "\n",
        "#we have dirs in our image directory so we use that, please use commented code if you directly have dataset images\n",
        "dirs = [f for f in listdir(IMAGE_PATH) if isdir(join(IMAGE_PATH, f))]\n",
        "files = [fl for direct in dirs for fl in listdir(join(IMAGE_PATH,direct))]\n",
        "# print(dirs, files)\n",
        "#files = [f for f in listdir(IMAGE_PATH) if isfile(join(IMAGE_PATH, f))]\n",
        "\n",
        "base_size = None\n",
        "for file in tqdm(files):\n",
        "  file2 = os.path.join(IMAGE_PATH,file)\n",
        "  try:\n",
        "    img = Image.open(file2)\n",
        "  except:\n",
        "    continue\n",
        "  sz = img.size\n",
        "  if base_size and sz!=base_size:\n",
        "    print(f\"Inconsistant size: {file2}\")\n",
        "  elif img.mode!='RGB':\n",
        "    print(f\"Inconsistant color format: {file2}\")\n",
        "  else:\n",
        "    base_size = sz\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7jBJ7przyqX"
      },
      "source": [
        "# start training from here if you have processed dataset (like us)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53TCFF_dx5kU",
        "outputId": "10b4dd76-6363-4d0f-c4e7-7b8ab6091289"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==1.8.1\n",
            "  Downloading torch-1.8.1-cp37-cp37m-manylinux1_x86_64.whl (804.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 804.1 MB 2.6 kB/s \n",
            "\u001b[?25hCollecting torchvision==0.9.1\n",
            "  Downloading torchvision-0.9.1-cp37-cp37m-manylinux1_x86_64.whl (17.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 17.4 MB 497 kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.8.1) (1.21.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.1) (3.10.0.2)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.9.1) (7.1.2)\n",
            "Installing collected packages: torch, torchvision\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.10.0+cu111\n",
            "    Uninstalling torch-1.10.0+cu111:\n",
            "      Successfully uninstalled torch-1.10.0+cu111\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.11.1+cu111\n",
            "    Uninstalling torchvision-0.11.1+cu111:\n",
            "      Successfully uninstalled torchvision-0.11.1+cu111\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.11.0 requires torch==1.10.0, but you have torch 1.8.1 which is incompatible.\n",
            "torchaudio 0.10.0+cu111 requires torch==1.10.0, but you have torch 1.8.1 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.8.1 torchvision-0.9.1\n"
          ]
        }
      ],
      "source": [
        "!pip install torch==1.8.1 torchvision==0.9.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "euJmsORUWRqB",
        "outputId": "586f1c12-471b-4db8-c4f5-62ad1f141a8f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting /content/stylegan2-ada-pytorch/training/networks.py\n"
          ]
        }
      ],
      "source": [
        "#@title modified architecture { display-mode: \"form\" }\n",
        "%%writefile /content/stylegan2-ada-pytorch/training/networks.py\n",
        "# Copyright (c) 2021, NVIDIA CORPORATION.  All rights reserved.\n",
        "#\n",
        "# NVIDIA CORPORATION and its licensors retain all intellectual property\n",
        "# and proprietary rights in and to this software, related documentation\n",
        "# and any modifications thereto.  Any use, reproduction, disclosure or\n",
        "# distribution of this software and related documentation without an express\n",
        "# license agreement from NVIDIA CORPORATION is strictly prohibited.\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch_utils import misc\n",
        "from torch_utils import persistence\n",
        "from torch_utils.ops import conv2d_resample\n",
        "from torch_utils.ops import upfirdn2d\n",
        "from torch_utils.ops import bias_act\n",
        "from torch_utils.ops import fma\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "@misc.profiled_function\n",
        "def normalize_2nd_moment(x, dim=1, eps=1e-8):\n",
        "    return x * (x.square().mean(dim=dim, keepdim=True) + eps).rsqrt()\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "@misc.profiled_function\n",
        "def modulated_conv2d(\n",
        "    x,                          # Input tensor of shape [batch_size, in_channels, in_height, in_width].\n",
        "    weight,                     # Weight tensor of shape [out_channels, in_channels, kernel_height, kernel_width].\n",
        "    styles,                     # Modulation coefficients of shape [batch_size, in_channels].\n",
        "    noise           = None,     # Optional noise tensor to add to the output activations.\n",
        "    up              = 1,        # Integer upsampling factor.\n",
        "    down            = 1,        # Integer downsampling factor.\n",
        "    padding         = 0,        # Padding with respect to the upsampled image.\n",
        "    resample_filter = None,     # Low-pass filter to apply when resampling activations. Must be prepared beforehand by calling upfirdn2d.setup_filter().\n",
        "    demodulate      = True,     # Apply weight demodulation?\n",
        "    flip_weight     = True,     # False = convolution, True = correlation (matches torch.nn.functional.conv2d).\n",
        "    fused_modconv   = True,     # Perform modulation, convolution, and demodulation as a single fused operation?\n",
        "):\n",
        "    batch_size = x.shape[0]\n",
        "    out_channels, in_channels, kh, kw = weight.shape\n",
        "    misc.assert_shape(weight, [out_channels, in_channels, kh, kw]) # [OIkk]\n",
        "    misc.assert_shape(x, [batch_size, in_channels, None, None]) # [NIHW]\n",
        "    misc.assert_shape(styles, [batch_size, in_channels]) # [NI]\n",
        "\n",
        "    # Pre-normalize inputs to avoid FP16 overflow.\n",
        "    if x.dtype == torch.float16 and demodulate:\n",
        "        weight = weight * (1 / np.sqrt(in_channels * kh * kw) / weight.norm(float('inf'), dim=[1,2,3], keepdim=True)) # max_Ikk\n",
        "        styles = styles / styles.norm(float('inf'), dim=1, keepdim=True) # max_I\n",
        "\n",
        "    # Calculate per-sample weights and demodulation coefficients.\n",
        "    w = None\n",
        "    dcoefs = None\n",
        "    if demodulate or fused_modconv:\n",
        "        w = weight.unsqueeze(0) # [NOIkk]\n",
        "        w = w * styles.reshape(batch_size, 1, -1, 1, 1) # [NOIkk]\n",
        "    if demodulate:\n",
        "        dcoefs = (w.square().sum(dim=[2,3,4]) + 1e-8).rsqrt() # [NO]\n",
        "    if demodulate and fused_modconv:\n",
        "        w = w * dcoefs.reshape(batch_size, -1, 1, 1, 1) # [NOIkk]\n",
        "\n",
        "    # Execute by scaling the activations before and after the convolution.\n",
        "    if not fused_modconv:\n",
        "        x = x * styles.to(x.dtype).reshape(batch_size, -1, 1, 1)\n",
        "        x = conv2d_resample.conv2d_resample(x=x, w=weight.to(x.dtype), f=resample_filter, up=up, down=down, padding=padding, flip_weight=flip_weight)\n",
        "        if demodulate and noise is not None:\n",
        "            x = fma.fma(x, dcoefs.to(x.dtype).reshape(batch_size, -1, 1, 1), noise.to(x.dtype))\n",
        "        elif demodulate:\n",
        "            x = x * dcoefs.to(x.dtype).reshape(batch_size, -1, 1, 1)\n",
        "        elif noise is not None:\n",
        "            x = x.add_(noise.to(x.dtype))\n",
        "        return x\n",
        "\n",
        "    # Execute as one fused op using grouped convolution.\n",
        "    with misc.suppress_tracer_warnings(): # this value will be treated as a constant\n",
        "        batch_size = int(batch_size)\n",
        "    misc.assert_shape(x, [batch_size, in_channels, None, None])\n",
        "    x = x.reshape(1, -1, *x.shape[2:])\n",
        "    w = w.reshape(-1, in_channels, kh, kw)\n",
        "    x = conv2d_resample.conv2d_resample(x=x, w=w.to(x.dtype), f=resample_filter, up=up, down=down, padding=padding, groups=batch_size, flip_weight=flip_weight)\n",
        "    x = x.reshape(batch_size, -1, *x.shape[2:])\n",
        "    if noise is not None:\n",
        "        x = x.add_(noise)\n",
        "    return x\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "@persistence.persistent_class\n",
        "class FullyConnectedLayer(torch.nn.Module):\n",
        "    def __init__(self,\n",
        "        in_features,                # Number of input features.\n",
        "        out_features,               # Number of output features.\n",
        "        bias            = True,     # Apply additive bias before the activation function?\n",
        "        activation      = 'linear', # Activation function: 'relu', 'lrelu', etc.\n",
        "        lr_multiplier   = 1,        # Learning rate multiplier.\n",
        "        bias_init       = 0,        # Initial value for the additive bias.\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.activation = activation\n",
        "        self.weight = torch.nn.Parameter(torch.randn([out_features, in_features]) / lr_multiplier)\n",
        "        self.bias = torch.nn.Parameter(torch.full([out_features], np.float32(bias_init))) if bias else None\n",
        "        self.weight_gain = lr_multiplier / np.sqrt(in_features)\n",
        "        self.bias_gain = lr_multiplier\n",
        "\n",
        "    def forward(self, x):\n",
        "        w = self.weight.to(x.dtype) * self.weight_gain\n",
        "        b = self.bias\n",
        "        if b is not None:\n",
        "            b = b.to(x.dtype)\n",
        "            if self.bias_gain != 1:\n",
        "                b = b * self.bias_gain\n",
        "\n",
        "        if self.activation == 'linear' and b is not None:\n",
        "            x = torch.addmm(b.unsqueeze(0), x, w.t())\n",
        "        else:\n",
        "            x = x.matmul(w.t())\n",
        "            x = bias_act.bias_act(x, b, act=self.activation)\n",
        "        return x\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "@persistence.persistent_class\n",
        "class Conv2dLayer(torch.nn.Module):\n",
        "    def __init__(self,\n",
        "        in_channels,                    # Number of input channels.\n",
        "        out_channels,                   # Number of output channels.\n",
        "        kernel_size,                    # Width and height of the convolution kernel.\n",
        "        bias            = True,         # Apply additive bias before the activation function?\n",
        "        activation      = 'linear',     # Activation function: 'relu', 'lrelu', etc.\n",
        "        up              = 1,            # Integer upsampling factor.\n",
        "        down            = 1,            # Integer downsampling factor.\n",
        "        resample_filter = [1,3,3,1],    # Low-pass filter to apply when resampling activations.\n",
        "        conv_clamp      = None,         # Clamp the output to +-X, None = disable clamping.\n",
        "        channels_last   = False,        # Expect the input to have memory_format=channels_last?\n",
        "        trainable       = True,         # Update the weights of this layer during training?\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.activation = activation\n",
        "        self.up = up\n",
        "        self.down = down\n",
        "        self.conv_clamp = conv_clamp\n",
        "        self.register_buffer('resample_filter', upfirdn2d.setup_filter(resample_filter))\n",
        "        self.padding = kernel_size // 2\n",
        "        self.weight_gain = 1 / np.sqrt(in_channels * (kernel_size ** 2))\n",
        "        self.act_gain = bias_act.activation_funcs[activation].def_gain\n",
        "\n",
        "        memory_format = torch.channels_last if channels_last else torch.contiguous_format\n",
        "        weight = torch.randn([out_channels, in_channels, kernel_size, kernel_size]).to(memory_format=memory_format)\n",
        "        bias = torch.zeros([out_channels]) if bias else None\n",
        "        if trainable:\n",
        "            self.weight = torch.nn.Parameter(weight)\n",
        "            self.bias = torch.nn.Parameter(bias) if bias is not None else None\n",
        "        else:\n",
        "            self.register_buffer('weight', weight)\n",
        "            if bias is not None:\n",
        "                self.register_buffer('bias', bias)\n",
        "            else:\n",
        "                self.bias = None\n",
        "\n",
        "    def forward(self, x, gain=1):\n",
        "        w = self.weight * self.weight_gain\n",
        "        b = self.bias.to(x.dtype) if self.bias is not None else None\n",
        "        flip_weight = (self.up == 1) # slightly faster\n",
        "        x = conv2d_resample.conv2d_resample(x=x, w=w.to(x.dtype), f=self.resample_filter, up=self.up, down=self.down, padding=self.padding, flip_weight=flip_weight)\n",
        "\n",
        "        act_gain = self.act_gain * gain\n",
        "        act_clamp = self.conv_clamp * gain if self.conv_clamp is not None else None\n",
        "        x = bias_act.bias_act(x, b, act=self.activation, gain=act_gain, clamp=act_clamp)\n",
        "        return x\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "@persistence.persistent_class\n",
        "class MappingNetwork(torch.nn.Module):\n",
        "    def __init__(self,\n",
        "        z_dim,                      # Input latent (Z) dimensionality, 0 = no latent.\n",
        "        c_dim,                      # Conditioning label (C) dimensionality, 0 = no label.\n",
        "        w_dim,                      # Intermediate latent (W) dimensionality.\n",
        "        num_ws,                     # Number of intermediate latents to output, None = do not broadcast.\n",
        "        num_layers      = 8,        # Number of mapping layers.\n",
        "        embed_features  = None,     # Label embedding dimensionality, None = same as w_dim.\n",
        "        layer_features  = None,     # Number of intermediate features in the mapping layers, None = same as w_dim.\n",
        "        activation      = 'lrelu',  # Activation function: 'relu', 'lrelu', etc.\n",
        "        lr_multiplier   = 0.01,     # Learning rate multiplier for the mapping layers.\n",
        "        w_avg_beta      = 0.995,    # Decay for tracking the moving average of W during training, None = do not track.\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.z_dim = z_dim\n",
        "        self.c_dim = c_dim\n",
        "        self.w_dim = w_dim\n",
        "        self.num_ws = num_ws\n",
        "        self.num_layers = num_layers\n",
        "        self.w_avg_beta = w_avg_beta\n",
        "\n",
        "        if embed_features is None:\n",
        "            embed_features = w_dim\n",
        "        if c_dim == 0:\n",
        "            embed_features = 0\n",
        "        if layer_features is None:\n",
        "            layer_features = w_dim\n",
        "        features_list = [z_dim + embed_features] + [layer_features] * (num_layers - 1) + [w_dim]\n",
        "\n",
        "        if c_dim > 0:\n",
        "            self.embed = FullyConnectedLayer(c_dim, embed_features)\n",
        "        for idx in range(num_layers):\n",
        "            in_features = features_list[idx]\n",
        "            out_features = features_list[idx + 1]\n",
        "            layer = FullyConnectedLayer(in_features, out_features, activation=activation, lr_multiplier=lr_multiplier)\n",
        "            setattr(self, f'fc{idx}', layer)\n",
        "\n",
        "        if num_ws is not None and w_avg_beta is not None:\n",
        "            self.register_buffer('w_avg', torch.zeros([w_dim]))\n",
        "\n",
        "    def forward(self, z, c, truncation_psi=1, truncation_cutoff=None, skip_w_avg_update=False):\n",
        "        # Embed, normalize, and concat inputs.\n",
        "        x = None\n",
        "        with torch.autograd.profiler.record_function('input'):\n",
        "            if self.z_dim > 0:\n",
        "                misc.assert_shape(z, [None, self.z_dim])\n",
        "                x = normalize_2nd_moment(z.to(torch.float32))\n",
        "            if self.c_dim > 0:\n",
        "                misc.assert_shape(c, [None, self.c_dim])\n",
        "                y = normalize_2nd_moment(self.embed(c.to(torch.float32)))\n",
        "                x = torch.cat([x, y], dim=1) if x is not None else y\n",
        "\n",
        "        # Main layers.\n",
        "        for idx in range(self.num_layers):\n",
        "            layer = getattr(self, f'fc{idx}')\n",
        "            x = layer(x)\n",
        "\n",
        "        # Update moving average of W.\n",
        "        if self.w_avg_beta is not None and self.training and not skip_w_avg_update:\n",
        "            with torch.autograd.profiler.record_function('update_w_avg'):\n",
        "                self.w_avg.copy_(x.detach().mean(dim=0).lerp(self.w_avg, self.w_avg_beta))\n",
        "\n",
        "        # Broadcast.\n",
        "        if self.num_ws is not None:\n",
        "            with torch.autograd.profiler.record_function('broadcast'):\n",
        "                x = x.unsqueeze(1).repeat([1, self.num_ws, 1])\n",
        "\n",
        "        # Apply truncation.\n",
        "        if truncation_psi != 1:\n",
        "            with torch.autograd.profiler.record_function('truncate'):\n",
        "                assert self.w_avg_beta is not None\n",
        "                if self.num_ws is None or truncation_cutoff is None:\n",
        "                    x = self.w_avg.lerp(x, truncation_psi)\n",
        "                else:\n",
        "                    x[:, :truncation_cutoff] = self.w_avg.lerp(x[:, :truncation_cutoff], truncation_psi)\n",
        "        return x\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "@persistence.persistent_class\n",
        "class SynthesisLayer(torch.nn.Module):\n",
        "    def __init__(self,\n",
        "        in_channels,                    # Number of input channels.\n",
        "        out_channels,                   # Number of output channels.\n",
        "        w_dim,                          # Intermediate latent (W) dimensionality.\n",
        "        resolution,                     # Resolution of this layer.\n",
        "        kernel_size     = 3,            # Convolution kernel size.\n",
        "        up              = 1,            # Integer upsampling factor.\n",
        "        use_noise       = True,         # Enable noise input?\n",
        "        activation      = 'lrelu',      # Activation function: 'relu', 'lrelu', etc.\n",
        "        resample_filter = [1,3,3,1],    # Low-pass filter to apply when resampling activations.\n",
        "        conv_clamp      = None,         # Clamp the output of convolution layers to +-X, None = disable clamping.\n",
        "        channels_last   = False,        # Use channels_last format for the weights?\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.resolution = resolution\n",
        "        self.up = up\n",
        "        self.use_noise = use_noise\n",
        "        self.activation = activation\n",
        "        self.conv_clamp = conv_clamp\n",
        "        self.register_buffer('resample_filter', upfirdn2d.setup_filter(resample_filter))\n",
        "        self.padding = kernel_size // 2\n",
        "        self.act_gain = bias_act.activation_funcs[activation].def_gain\n",
        "\n",
        "        self.affine = FullyConnectedLayer(w_dim, in_channels, bias_init=1)\n",
        "        memory_format = torch.channels_last if channels_last else torch.contiguous_format\n",
        "        self.weight = torch.nn.Parameter(torch.randn([out_channels, in_channels, kernel_size, kernel_size]).to(memory_format=memory_format))\n",
        "        if use_noise:\n",
        "            self.register_buffer('noise_const', torch.randn([resolution, resolution]))\n",
        "            self.noise_strength = torch.nn.Parameter(torch.zeros([]))\n",
        "        self.bias = torch.nn.Parameter(torch.zeros([out_channels]))\n",
        "\n",
        "    def forward(self, x, w, noise_mode='random', fused_modconv=True, gain=1):\n",
        "        assert noise_mode in ['random', 'const', 'none']\n",
        "        in_resolution = self.resolution // self.up\n",
        "        misc.assert_shape(x, [None, self.weight.shape[1], in_resolution, in_resolution])\n",
        "        styles = self.affine(w)\n",
        "\n",
        "        noise = None\n",
        "        if self.use_noise and noise_mode == 'random':\n",
        "            noise = torch.randn([x.shape[0], 1, self.resolution, self.resolution], device=x.device) * self.noise_strength\n",
        "        if self.use_noise and noise_mode == 'const':\n",
        "            noise = self.noise_const * self.noise_strength\n",
        "\n",
        "        flip_weight = (self.up == 1) # slightly faster\n",
        "        x = modulated_conv2d(x=x, weight=self.weight, styles=styles, noise=noise, up=self.up,\n",
        "            padding=self.padding, resample_filter=self.resample_filter, flip_weight=flip_weight, fused_modconv=fused_modconv)\n",
        "\n",
        "        act_gain = self.act_gain * gain\n",
        "        act_clamp = self.conv_clamp * gain if self.conv_clamp is not None else None\n",
        "        x = bias_act.bias_act(x, self.bias.to(x.dtype), act=self.activation, gain=act_gain, clamp=act_clamp)\n",
        "        return x\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "@persistence.persistent_class\n",
        "class ToRGBLayer(torch.nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, w_dim, kernel_size=1, conv_clamp=None, channels_last=False):\n",
        "        super().__init__()\n",
        "        self.conv_clamp = conv_clamp\n",
        "        self.affine = FullyConnectedLayer(w_dim, in_channels, bias_init=1)\n",
        "        memory_format = torch.channels_last if channels_last else torch.contiguous_format\n",
        "        self.weight = torch.nn.Parameter(torch.randn([out_channels, in_channels, kernel_size, kernel_size]).to(memory_format=memory_format))\n",
        "        self.bias = torch.nn.Parameter(torch.zeros([out_channels]))\n",
        "        self.weight_gain = 1 / np.sqrt(in_channels * (kernel_size ** 2))\n",
        "\n",
        "    def forward(self, x, w, fused_modconv=True):\n",
        "        styles = self.affine(w) * self.weight_gain\n",
        "        x = modulated_conv2d(x=x, weight=self.weight, styles=styles, demodulate=False, fused_modconv=fused_modconv)\n",
        "        x = bias_act.bias_act(x, self.bias.to(x.dtype), clamp=self.conv_clamp)\n",
        "        return x\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "@persistence.persistent_class\n",
        "class SynthesisBlock(torch.nn.Module):\n",
        "    def __init__(self,\n",
        "        in_channels,                        # Number of input channels, 0 = first block.\n",
        "        out_channels,                       # Number of output channels.\n",
        "        w_dim,                              # Intermediate latent (W) dimensionality.\n",
        "        resolution,                         # Resolution of this block.\n",
        "        img_channels,                       # Number of output color channels.\n",
        "        is_last,                            # Is this the last block?\n",
        "        architecture        = 'resnet',       # Architecture: 'orig', 'skip', 'resnet'.\n",
        "        resample_filter     = [1,3,3,1],    # Low-pass filter to apply when resampling activations.\n",
        "        conv_clamp          = None,         # Clamp the output of convolution layers to +-X, None = disable clamping.\n",
        "        use_fp16            = False,        # Use FP16 for this block?\n",
        "        fp16_channels_last  = False,        # Use channels-last memory format with FP16?\n",
        "        **layer_kwargs,                     # Arguments for SynthesisLayer.\n",
        "    ):\n",
        "        assert architecture in ['orig', 'skip', 'resnet']\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.w_dim = w_dim\n",
        "        self.resolution = resolution\n",
        "        self.img_channels = img_channels\n",
        "        self.is_last = is_last\n",
        "        self.architecture = architecture\n",
        "        self.use_fp16 = use_fp16\n",
        "        self.channels_last = (use_fp16 and fp16_channels_last)\n",
        "        self.register_buffer('resample_filter', upfirdn2d.setup_filter(resample_filter))\n",
        "        self.num_conv = 0\n",
        "        self.num_torgb = 0\n",
        "\n",
        "        if in_channels == 0:\n",
        "            self.const = torch.nn.Parameter(torch.randn([out_channels, resolution, resolution]))\n",
        "\n",
        "        if in_channels != 0:\n",
        "            self.conv0 = SynthesisLayer(in_channels, out_channels, w_dim=w_dim, resolution=resolution, up=2,\n",
        "                resample_filter=resample_filter, conv_clamp=conv_clamp, channels_last=self.channels_last, **layer_kwargs)\n",
        "            self.num_conv += 1\n",
        "\n",
        "        self.conv1 = SynthesisLayer(out_channels, out_channels, w_dim=w_dim, resolution=resolution,\n",
        "            conv_clamp=conv_clamp, channels_last=self.channels_last, **layer_kwargs)\n",
        "        self.num_conv += 1\n",
        "\n",
        "        if is_last or architecture == 'skip':\n",
        "            self.torgb = ToRGBLayer(out_channels, img_channels, w_dim=w_dim,\n",
        "                conv_clamp=conv_clamp, channels_last=self.channels_last)\n",
        "            self.num_torgb += 1\n",
        "\n",
        "        if in_channels != 0 and architecture == 'resnet':\n",
        "            self.skip = Conv2dLayer(in_channels, out_channels, kernel_size=1, bias=False, up=2,\n",
        "                resample_filter=resample_filter, channels_last=self.channels_last)\n",
        "\n",
        "    def forward(self, x, img, ws, force_fp32=False, fused_modconv=None, **layer_kwargs):\n",
        "        misc.assert_shape(ws, [None, self.num_conv + self.num_torgb, self.w_dim])\n",
        "        w_iter = iter(ws.unbind(dim=1))\n",
        "        dtype = torch.float16 if self.use_fp16 and not force_fp32 else torch.float32\n",
        "        memory_format = torch.channels_last if self.channels_last and not force_fp32 else torch.contiguous_format\n",
        "        if fused_modconv is None:\n",
        "            with misc.suppress_tracer_warnings(): # this value will be treated as a constant\n",
        "                fused_modconv = (not self.training) and (dtype == torch.float32 or int(x.shape[0]) == 1)\n",
        "\n",
        "        # Input.\n",
        "        if self.in_channels == 0:\n",
        "            x = self.const.to(dtype=dtype, memory_format=memory_format)\n",
        "            x = x.unsqueeze(0).repeat([ws.shape[0], 1, 1, 1])\n",
        "        else:\n",
        "            misc.assert_shape(x, [None, self.in_channels, self.resolution // 2, self.resolution // 2])\n",
        "            x = x.to(dtype=dtype, memory_format=memory_format)\n",
        "\n",
        "        # Main layers.\n",
        "        if self.in_channels == 0:\n",
        "            x = self.conv1(x, next(w_iter), fused_modconv=fused_modconv, **layer_kwargs)\n",
        "        elif self.architecture == 'resnet':\n",
        "            y = self.skip(x, gain=np.sqrt(0.5))\n",
        "            x = self.conv0(x, next(w_iter), fused_modconv=fused_modconv, **layer_kwargs)\n",
        "            x = self.conv1(x, next(w_iter), fused_modconv=fused_modconv, gain=np.sqrt(0.5), **layer_kwargs)\n",
        "            x = y.add_(x)\n",
        "        else:\n",
        "            x = self.conv0(x, next(w_iter), fused_modconv=fused_modconv, **layer_kwargs)\n",
        "            x = self.conv1(x, next(w_iter), fused_modconv=fused_modconv, **layer_kwargs)\n",
        "\n",
        "        # ToRGB.\n",
        "        if img is not None:\n",
        "            misc.assert_shape(img, [None, self.img_channels, self.resolution // 2, self.resolution // 2])\n",
        "            img = upfirdn2d.upsample2d(img, self.resample_filter)\n",
        "        if self.is_last or self.architecture == 'skip':\n",
        "            y = self.torgb(x, next(w_iter), fused_modconv=fused_modconv)\n",
        "            y = y.to(dtype=torch.float32, memory_format=torch.contiguous_format)\n",
        "            img = img.add_(y) if img is not None else y\n",
        "\n",
        "        assert x.dtype == dtype\n",
        "        assert img is None or img.dtype == torch.float32\n",
        "        return x, img\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "@persistence.persistent_class\n",
        "class SynthesisNetwork(torch.nn.Module):\n",
        "    def __init__(self,\n",
        "        w_dim,                      # Intermediate latent (W) dimensionality.\n",
        "        img_resolution,             # Output image resolution.\n",
        "        img_channels,               # Number of color channels.\n",
        "        channel_base    = 32768,    # Overall multiplier for the number of channels.\n",
        "        channel_max     = 512,      # Maximum number of channels in any layer.\n",
        "        num_fp16_res    = 0,        # Use FP16 for the N highest resolutions.\n",
        "        **block_kwargs,             # Arguments for SynthesisBlock.\n",
        "    ):\n",
        "        assert img_resolution >= 4 and img_resolution & (img_resolution - 1) == 0\n",
        "        super().__init__()\n",
        "        self.w_dim = w_dim\n",
        "        self.img_resolution = img_resolution\n",
        "        self.img_resolution_log2 = int(np.log2(img_resolution))\n",
        "        self.img_channels = img_channels\n",
        "        self.block_resolutions = [2 ** i for i in range(2, self.img_resolution_log2 + 1)]\n",
        "        channels_dict = {res: min(channel_base // res, channel_max) for res in self.block_resolutions}\n",
        "        fp16_resolution = max(2 ** (self.img_resolution_log2 + 1 - num_fp16_res), 8)\n",
        "\n",
        "        self.num_ws = 0\n",
        "        for res in self.block_resolutions:\n",
        "            in_channels = channels_dict[res // 2] if res > 4 else 0\n",
        "            out_channels = channels_dict[res]\n",
        "            use_fp16 = (res >= fp16_resolution)\n",
        "            is_last = (res == self.img_resolution)\n",
        "            block = SynthesisBlock(in_channels, out_channels, w_dim=w_dim, resolution=res,\n",
        "                img_channels=img_channels, is_last=is_last, use_fp16=use_fp16, **block_kwargs)\n",
        "            self.num_ws += block.num_conv\n",
        "            if is_last:\n",
        "                self.num_ws += block.num_torgb\n",
        "            setattr(self, f'b{res}', block)\n",
        "\n",
        "    def forward(self, ws, **block_kwargs):\n",
        "        block_ws = []\n",
        "        with torch.autograd.profiler.record_function('split_ws'):\n",
        "            misc.assert_shape(ws, [None, self.num_ws, self.w_dim])\n",
        "            ws = ws.to(torch.float32)\n",
        "            w_idx = 0\n",
        "            for res in self.block_resolutions:\n",
        "                block = getattr(self, f'b{res}')\n",
        "                block_ws.append(ws.narrow(1, w_idx, block.num_conv + block.num_torgb))\n",
        "                w_idx += block.num_conv\n",
        "\n",
        "        x = img = None\n",
        "        for res, cur_ws in zip(self.block_resolutions, block_ws):\n",
        "            block = getattr(self, f'b{res}')\n",
        "            x, img = block(x, img, cur_ws, **block_kwargs)\n",
        "        return img\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "@persistence.persistent_class\n",
        "class Generator(torch.nn.Module):\n",
        "    def __init__(self,\n",
        "        z_dim,                      # Input latent (Z) dimensionality.\n",
        "        c_dim,                      # Conditioning label (C) dimensionality.\n",
        "        w_dim,                      # Intermediate latent (W) dimensionality.\n",
        "        img_resolution,             # Output resolution.\n",
        "        img_channels,               # Number of output color channels.\n",
        "        mapping_kwargs      = {},   # Arguments for MappingNetwork.\n",
        "        synthesis_kwargs    = {},   # Arguments for SynthesisNetwork.\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.z_dim = z_dim\n",
        "        self.c_dim = c_dim\n",
        "        self.w_dim = w_dim\n",
        "        self.img_resolution = img_resolution\n",
        "        self.img_channels = img_channels\n",
        "        self.synthesis = SynthesisNetwork(w_dim=w_dim, img_resolution=img_resolution, img_channels=img_channels, **synthesis_kwargs)\n",
        "        self.num_ws = self.synthesis.num_ws\n",
        "        self.mapping = MappingNetwork(z_dim=z_dim, c_dim=c_dim, w_dim=w_dim, num_ws=self.num_ws, **mapping_kwargs)\n",
        "\n",
        "    def forward(self, z, c, truncation_psi=1, truncation_cutoff=None, **synthesis_kwargs):\n",
        "        ws = self.mapping(z, c, truncation_psi=truncation_psi, truncation_cutoff=truncation_cutoff)\n",
        "        img = self.synthesis(ws, **synthesis_kwargs)\n",
        "        return img\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "@persistence.persistent_class\n",
        "class DiscriminatorBlock(torch.nn.Module):\n",
        "    def __init__(self,\n",
        "        in_channels,                        # Number of input channels, 0 = first block.\n",
        "        tmp_channels,                       # Number of intermediate channels.\n",
        "        out_channels,                       # Number of output channels.\n",
        "        resolution,                         # Resolution of this block.\n",
        "        img_channels,                       # Number of input color channels.\n",
        "        first_layer_idx,                    # Index of the first layer.\n",
        "        architecture        = 'resnet',     # Architecture: 'orig', 'skip', 'resnet'.\n",
        "        activation          = 'lrelu',      # Activation function: 'relu', 'lrelu', etc.\n",
        "        resample_filter     = [1,3,3,1],    # Low-pass filter to apply when resampling activations.\n",
        "        conv_clamp          = None,         # Clamp the output of convolution layers to +-X, None = disable clamping.\n",
        "        use_fp16            = False,        # Use FP16 for this block?\n",
        "        fp16_channels_last  = False,        # Use channels-last memory format with FP16?\n",
        "        freeze_layers       = 0,            # Freeze-D: Number of layers to freeze.\n",
        "    ):\n",
        "        assert in_channels in [0, tmp_channels]\n",
        "        assert architecture in ['orig', 'skip', 'resnet']\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.resolution = resolution\n",
        "        self.img_channels = img_channels\n",
        "        self.first_layer_idx = first_layer_idx\n",
        "        self.architecture = architecture\n",
        "        self.use_fp16 = use_fp16\n",
        "        self.channels_last = (use_fp16 and fp16_channels_last)\n",
        "        self.register_buffer('resample_filter', upfirdn2d.setup_filter(resample_filter))\n",
        "\n",
        "        self.num_layers = 0\n",
        "        def trainable_gen():\n",
        "            while True:\n",
        "                layer_idx = self.first_layer_idx + self.num_layers\n",
        "                trainable = (layer_idx >= freeze_layers)\n",
        "                self.num_layers += 1\n",
        "                yield trainable\n",
        "        trainable_iter = trainable_gen()\n",
        "\n",
        "        if in_channels == 0 or architecture == 'skip':\n",
        "            self.fromrgb = Conv2dLayer(img_channels, tmp_channels, kernel_size=1, activation=activation,\n",
        "                trainable=next(trainable_iter), conv_clamp=conv_clamp, channels_last=self.channels_last)\n",
        "\n",
        "        self.conv0 = Conv2dLayer(tmp_channels, tmp_channels, kernel_size=3, activation=activation,\n",
        "            trainable=next(trainable_iter), conv_clamp=conv_clamp, channels_last=self.channels_last)\n",
        "\n",
        "        self.conv1 = Conv2dLayer(tmp_channels, out_channels, kernel_size=3, activation=activation, down=2,\n",
        "            trainable=next(trainable_iter), resample_filter=resample_filter, conv_clamp=conv_clamp, channels_last=self.channels_last)\n",
        "\n",
        "        if architecture == 'resnet':\n",
        "            self.skip = Conv2dLayer(tmp_channels, out_channels, kernel_size=1, bias=False, down=2,\n",
        "                trainable=next(trainable_iter), resample_filter=resample_filter, channels_last=self.channels_last)\n",
        "\n",
        "    def forward(self, x, img, force_fp32=False):\n",
        "        dtype = torch.float16 if self.use_fp16 and not force_fp32 else torch.float32\n",
        "        memory_format = torch.channels_last if self.channels_last and not force_fp32 else torch.contiguous_format\n",
        "\n",
        "        # Input.\n",
        "        if x is not None:\n",
        "            misc.assert_shape(x, [None, self.in_channels, self.resolution, self.resolution])\n",
        "            x = x.to(dtype=dtype, memory_format=memory_format)\n",
        "\n",
        "        # FromRGB.\n",
        "        if self.in_channels == 0 or self.architecture == 'skip':\n",
        "            misc.assert_shape(img, [None, self.img_channels, self.resolution, self.resolution])\n",
        "            img = img.to(dtype=dtype, memory_format=memory_format)\n",
        "            y = self.fromrgb(img)\n",
        "            x = x + y if x is not None else y\n",
        "            img = upfirdn2d.downsample2d(img, self.resample_filter) if self.architecture == 'skip' else None\n",
        "\n",
        "        # Main layers.\n",
        "        if self.architecture == 'resnet':\n",
        "            y = self.skip(x, gain=np.sqrt(0.5))\n",
        "            x = self.conv0(x)\n",
        "            x = self.conv1(x, gain=np.sqrt(0.5))\n",
        "            x = y.add_(x)\n",
        "        else:\n",
        "            x = self.conv0(x)\n",
        "            x = self.conv1(x)\n",
        "\n",
        "        assert x.dtype == dtype\n",
        "        return x, img\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "@persistence.persistent_class\n",
        "class MinibatchStdLayer(torch.nn.Module):\n",
        "    def __init__(self, group_size, num_channels=1):\n",
        "        super().__init__()\n",
        "        self.group_size = group_size\n",
        "        self.num_channels = num_channels\n",
        "\n",
        "    def forward(self, x):\n",
        "        N, C, H, W = x.shape\n",
        "        with misc.suppress_tracer_warnings(): # as_tensor results are registered as constants\n",
        "            G = torch.min(torch.as_tensor(self.group_size), torch.as_tensor(N)) if self.group_size is not None else N\n",
        "        F = self.num_channels\n",
        "        c = C // F\n",
        "\n",
        "        y = x.reshape(G, -1, F, c, H, W)    # [GnFcHW] Split minibatch N into n groups of size G, and channels C into F groups of size c.\n",
        "        y = y - y.mean(dim=0)               # [GnFcHW] Subtract mean over group.\n",
        "        y = y.square().mean(dim=0)          # [nFcHW]  Calc variance over group.\n",
        "        y = (y + 1e-8).sqrt()               # [nFcHW]  Calc stddev over group.\n",
        "        y = y.mean(dim=[2,3,4])             # [nF]     Take average over channels and pixels.\n",
        "        y = y.reshape(-1, F, 1, 1)          # [nF11]   Add missing dimensions.\n",
        "        y = y.repeat(G, 1, H, W)            # [NFHW]   Replicate over group and pixels.\n",
        "        x = torch.cat([x, y], dim=1)        # [NCHW]   Append to input as new channels.\n",
        "        return x\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "@persistence.persistent_class\n",
        "class DiscriminatorEpilogue(torch.nn.Module):\n",
        "    def __init__(self,\n",
        "        in_channels,                    # Number of input channels.\n",
        "        cmap_dim,                       # Dimensionality of mapped conditioning label, 0 = no label.\n",
        "        resolution,                     # Resolution of this block.\n",
        "        img_channels,                   # Number of input color channels.\n",
        "        architecture        = 'resnet', # Architecture: 'orig', 'skip', 'resnet'.\n",
        "        mbstd_group_size    = 4,        # Group size for the minibatch standard deviation layer, None = entire minibatch.\n",
        "        mbstd_num_channels  = 1,        # Number of features for the minibatch standard deviation layer, 0 = disable.\n",
        "        activation          = 'lrelu',  # Activation function: 'relu', 'lrelu', etc.\n",
        "        conv_clamp          = None,     # Clamp the output of convolution layers to +-X, None = disable clamping.\n",
        "    ):\n",
        "        assert architecture in ['orig', 'skip', 'resnet']\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.cmap_dim = cmap_dim\n",
        "        self.resolution = resolution\n",
        "        self.img_channels = img_channels\n",
        "        self.architecture = architecture\n",
        "\n",
        "        if architecture == 'skip':\n",
        "            self.fromrgb = Conv2dLayer(img_channels, in_channels, kernel_size=1, activation=activation)\n",
        "        self.mbstd = MinibatchStdLayer(group_size=mbstd_group_size, num_channels=mbstd_num_channels) if mbstd_num_channels > 0 else None\n",
        "        self.conv = Conv2dLayer(in_channels + mbstd_num_channels, in_channels, kernel_size=3, activation=activation, conv_clamp=conv_clamp)\n",
        "        self.fc = FullyConnectedLayer(in_channels * (resolution ** 2), in_channels, activation=activation)\n",
        "        self.out = FullyConnectedLayer(in_channels, 1 if cmap_dim == 0 else cmap_dim)\n",
        "\n",
        "    def forward(self, x, img, cmap, force_fp32=False):\n",
        "        misc.assert_shape(x, [None, self.in_channels, self.resolution, self.resolution]) # [NCHW]\n",
        "        _ = force_fp32 # unused\n",
        "        dtype = torch.float32\n",
        "        memory_format = torch.contiguous_format\n",
        "\n",
        "        # FromRGB.\n",
        "        x = x.to(dtype=dtype, memory_format=memory_format)\n",
        "        if self.architecture == 'skip':\n",
        "            misc.assert_shape(img, [None, self.img_channels, self.resolution, self.resolution])\n",
        "            img = img.to(dtype=dtype, memory_format=memory_format)\n",
        "            x = x + self.fromrgb(img)\n",
        "\n",
        "        # Main layers.\n",
        "        if self.mbstd is not None:\n",
        "            x = self.mbstd(x)\n",
        "        x = self.conv(x)\n",
        "        x = self.fc(x.flatten(1))\n",
        "        x = self.out(x)\n",
        "\n",
        "        # Conditioning.\n",
        "        if self.cmap_dim > 0:\n",
        "            misc.assert_shape(cmap, [None, self.cmap_dim])\n",
        "            x = (x * cmap).sum(dim=1, keepdim=True) * (1 / np.sqrt(self.cmap_dim))\n",
        "\n",
        "        assert x.dtype == dtype\n",
        "        return x\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "@persistence.persistent_class\n",
        "class Discriminator(torch.nn.Module):\n",
        "    def __init__(self,\n",
        "        c_dim,                          # Conditioning label (C) dimensionality.\n",
        "        img_resolution,                 # Input resolution.\n",
        "        img_channels,                   # Number of input color channels.\n",
        "        architecture        = 'resnet', # Architecture: 'orig', 'skip', 'resnet'.\n",
        "        channel_base        = 32768,    # Overall multiplier for the number of channels.\n",
        "        channel_max         = 512,      # Maximum number of channels in any layer.\n",
        "        num_fp16_res        = 0,        # Use FP16 for the N highest resolutions.\n",
        "        conv_clamp          = None,     # Clamp the output of convolution layers to +-X, None = disable clamping.\n",
        "        cmap_dim            = None,     # Dimensionality of mapped conditioning label, None = default.\n",
        "        block_kwargs        = {},       # Arguments for DiscriminatorBlock.\n",
        "        mapping_kwargs      = {},       # Arguments for MappingNetwork.\n",
        "        epilogue_kwargs     = {},       # Arguments for DiscriminatorEpilogue.\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.c_dim = c_dim\n",
        "        self.img_resolution = img_resolution\n",
        "        self.img_resolution_log2 = int(np.log2(img_resolution))\n",
        "        self.img_channels = img_channels\n",
        "        self.block_resolutions = [2 ** i for i in range(self.img_resolution_log2, 2, -1)]\n",
        "        channels_dict = {res: min(channel_base // res, channel_max) for res in self.block_resolutions + [4]}\n",
        "        fp16_resolution = max(2 ** (self.img_resolution_log2 + 1 - num_fp16_res), 8)\n",
        "\n",
        "        if cmap_dim is None:\n",
        "            cmap_dim = channels_dict[4]\n",
        "        if c_dim == 0:\n",
        "            cmap_dim = 0\n",
        "\n",
        "        common_kwargs = dict(img_channels=img_channels, architecture=architecture, conv_clamp=conv_clamp)\n",
        "        cur_layer_idx = 0\n",
        "        for res in self.block_resolutions:\n",
        "            in_channels = channels_dict[res] if res < img_resolution else 0\n",
        "            tmp_channels = channels_dict[res]\n",
        "            out_channels = channels_dict[res // 2]\n",
        "            use_fp16 = (res >= fp16_resolution)\n",
        "            block = DiscriminatorBlock(in_channels, tmp_channels, out_channels, resolution=res,\n",
        "                first_layer_idx=cur_layer_idx, use_fp16=use_fp16, **block_kwargs, **common_kwargs)\n",
        "            setattr(self, f'b{res}', block)\n",
        "            cur_layer_idx += block.num_layers\n",
        "        if c_dim > 0:\n",
        "            self.mapping = MappingNetwork(z_dim=0, c_dim=c_dim, w_dim=cmap_dim, num_ws=None, w_avg_beta=None, **mapping_kwargs)\n",
        "        self.b4 = DiscriminatorEpilogue(channels_dict[4], cmap_dim=cmap_dim, resolution=4, **epilogue_kwargs, **common_kwargs)\n",
        "\n",
        "    def forward(self, img, c, **block_kwargs):\n",
        "        x = None\n",
        "        for res in self.block_resolutions:\n",
        "            block = getattr(self, f'b{res}')\n",
        "            x, img = block(x, img, **block_kwargs)\n",
        "\n",
        "        cmap = None\n",
        "        if self.c_dim > 0:\n",
        "            cmap = self.mapping(None, c)\n",
        "        x = self.b4(x, img, cmap)\n",
        "        return x\n",
        "\n",
        "#----------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5No-bokaG5Ed"
      },
      "source": [
        "# Perform Initial Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UZ5xCUMnsgEp"
      },
      "outputs": [],
      "source": [
        "!rm -r /content/drive/MyDrive/Datasets/Kaggle/Landscape/high_res_with_aug_experiments/00008-High_res_Processed_mountain-auto1-kimg200-batch8-bgcfnc-resumecustom"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tzAdHMp7KLzz",
        "outputId": "a4cd4ec5-6a59-443d-fe2d-297af2eb8192"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training options:\n",
            "{\n",
            "  \"num_gpus\": 1,\n",
            "  \"image_snapshot_ticks\": 10,\n",
            "  \"network_snapshot_ticks\": 10,\n",
            "  \"metrics\": [\n",
            "    \"fid50k_full\"\n",
            "  ],\n",
            "  \"random_seed\": 0,\n",
            "  \"training_set_kwargs\": {\n",
            "    \"class_name\": \"training.dataset.ImageFolderDataset\",\n",
            "    \"path\": \"/content/drive/MyDrive/Landscape/seg_train/seg_train/High_res_Processed_mountain\",\n",
            "    \"use_labels\": false,\n",
            "    \"max_size\": 2512,\n",
            "    \"xflip\": false,\n",
            "    \"resolution\": 1024\n",
            "  },\n",
            "  \"data_loader_kwargs\": {\n",
            "    \"pin_memory\": true,\n",
            "    \"num_workers\": 3,\n",
            "    \"prefetch_factor\": 2\n",
            "  },\n",
            "  \"G_kwargs\": {\n",
            "    \"class_name\": \"training.networks.Generator\",\n",
            "    \"z_dim\": 512,\n",
            "    \"w_dim\": 512,\n",
            "    \"mapping_kwargs\": {\n",
            "      \"num_layers\": 2\n",
            "    },\n",
            "    \"synthesis_kwargs\": {\n",
            "      \"channel_base\": 32768,\n",
            "      \"channel_max\": 512,\n",
            "      \"num_fp16_res\": 4,\n",
            "      \"conv_clamp\": 256\n",
            "    }\n",
            "  },\n",
            "  \"D_kwargs\": {\n",
            "    \"class_name\": \"training.networks.Discriminator\",\n",
            "    \"block_kwargs\": {},\n",
            "    \"mapping_kwargs\": {},\n",
            "    \"epilogue_kwargs\": {\n",
            "      \"mbstd_group_size\": 4\n",
            "    },\n",
            "    \"channel_base\": 32768,\n",
            "    \"channel_max\": 512,\n",
            "    \"num_fp16_res\": 4,\n",
            "    \"conv_clamp\": 256\n",
            "  },\n",
            "  \"G_opt_kwargs\": {\n",
            "    \"class_name\": \"torch.optim.Adam\",\n",
            "    \"lr\": 0.002,\n",
            "    \"betas\": [\n",
            "      0,\n",
            "      0.99\n",
            "    ],\n",
            "    \"eps\": 1e-08\n",
            "  },\n",
            "  \"D_opt_kwargs\": {\n",
            "    \"class_name\": \"torch.optim.Adam\",\n",
            "    \"lr\": 0.002,\n",
            "    \"betas\": [\n",
            "      0,\n",
            "      0.99\n",
            "    ],\n",
            "    \"eps\": 1e-08\n",
            "  },\n",
            "  \"loss_kwargs\": {\n",
            "    \"class_name\": \"training.loss.StyleGAN2Loss\",\n",
            "    \"r1_gamma\": 52.4288\n",
            "  },\n",
            "  \"total_kimg\": 300,\n",
            "  \"batch_size\": 8,\n",
            "  \"batch_gpu\": 8,\n",
            "  \"ema_kimg\": 1.25,\n",
            "  \"ema_rampup\": 0.05,\n",
            "  \"ada_target\": 0.6,\n",
            "  \"augment_kwargs\": {\n",
            "    \"class_name\": \"training.augment.AugmentPipe\",\n",
            "    \"xflip\": 1,\n",
            "    \"rotate90\": 1,\n",
            "    \"xint\": 1,\n",
            "    \"scale\": 1,\n",
            "    \"rotate\": 1,\n",
            "    \"aniso\": 1,\n",
            "    \"xfrac\": 1,\n",
            "    \"brightness\": 1,\n",
            "    \"contrast\": 1,\n",
            "    \"lumaflip\": 1,\n",
            "    \"hue\": 1,\n",
            "    \"saturation\": 1\n",
            "  },\n",
            "  \"run_dir\": \"/content/drive/MyDrive/Landscape/high_res_baseline_experiments/00014-High_res_Processed_mountain-auto1-kimg300-batch8-ada\"\n",
            "}\n",
            "\n",
            "Output directory:   /content/drive/MyDrive/Landscape/high_res_baseline_experiments/00014-High_res_Processed_mountain-auto1-kimg300-batch8-ada\n",
            "Training data:      /content/drive/MyDrive/Landscape/seg_train/seg_train/High_res_Processed_mountain\n",
            "Training duration:  300 kimg\n",
            "Number of GPUs:     1\n",
            "Number of images:   2512\n",
            "Image resolution:   1024\n",
            "Conditional model:  False\n",
            "Dataset x-flips:    False\n",
            "\n",
            "Creating output directory...\n",
            "Launching processes...\n",
            "Loading training set...\n",
            "\n",
            "Num images:  2512\n",
            "Image shape: [3, 1024, 1024]\n",
            "Label shape: [0]\n",
            "\n",
            "Constructing networks...\n",
            "Setting up PyTorch plugin \"bias_act_plugin\"... Done.\n",
            "Setting up PyTorch plugin \"upfirdn2d_plugin\"... Done.\n",
            "\n",
            "Generator              Parameters  Buffers  Output shape         Datatype\n",
            "---                    ---         ---      ---                  ---     \n",
            "mapping.fc0            262656      -        [8, 512]             float32 \n",
            "mapping.fc1            262656      -        [8, 512]             float32 \n",
            "mapping                -           512      [8, 18, 512]         float32 \n",
            "synthesis.b4.conv1     2622465     32       [8, 512, 4, 4]       float32 \n",
            "synthesis.b4.torgb     264195      -        [8, 3, 4, 4]         float32 \n",
            "synthesis.b4:0         8192        16       [8, 512, 4, 4]       float32 \n",
            "synthesis.b4:1         -           -        [8, 512, 4, 4]       float32 \n",
            "synthesis.b8.conv0     2622465     80       [8, 512, 8, 8]       float32 \n",
            "synthesis.b8.conv1     2622465     80       [8, 512, 8, 8]       float32 \n",
            "synthesis.b8.torgb     264195      -        [8, 3, 8, 8]         float32 \n",
            "synthesis.b8:0         -           16       [8, 512, 8, 8]       float32 \n",
            "synthesis.b8:1         -           -        [8, 512, 8, 8]       float32 \n",
            "synthesis.b16.conv0    2622465     272      [8, 512, 16, 16]     float32 \n",
            "synthesis.b16.conv1    2622465     272      [8, 512, 16, 16]     float32 \n",
            "synthesis.b16.torgb    264195      -        [8, 3, 16, 16]       float32 \n",
            "synthesis.b16:0        -           16       [8, 512, 16, 16]     float32 \n",
            "synthesis.b16:1        -           -        [8, 512, 16, 16]     float32 \n",
            "synthesis.b32.conv0    2622465     1040     [8, 512, 32, 32]     float32 \n",
            "synthesis.b32.conv1    2622465     1040     [8, 512, 32, 32]     float32 \n",
            "synthesis.b32.torgb    264195      -        [8, 3, 32, 32]       float32 \n",
            "synthesis.b32:0        -           16       [8, 512, 32, 32]     float32 \n",
            "synthesis.b32:1        -           -        [8, 512, 32, 32]     float32 \n",
            "synthesis.b64.conv0    2622465     4112     [8, 512, 64, 64]     float32 \n",
            "synthesis.b64.conv1    2622465     4112     [8, 512, 64, 64]     float32 \n",
            "synthesis.b64.torgb    264195      -        [8, 3, 64, 64]       float32 \n",
            "synthesis.b64:0        -           16       [8, 512, 64, 64]     float32 \n",
            "synthesis.b64:1        -           -        [8, 512, 64, 64]     float32 \n",
            "synthesis.b128.conv0   1442561     16400    [8, 256, 128, 128]   float16 \n",
            "synthesis.b128.conv1   721409      16400    [8, 256, 128, 128]   float16 \n",
            "synthesis.b128.torgb   132099      -        [8, 3, 128, 128]     float16 \n",
            "synthesis.b128:0       -           16       [8, 256, 128, 128]   float16 \n",
            "synthesis.b128:1       -           -        [8, 256, 128, 128]   float32 \n",
            "synthesis.b256.conv0   426369      65552    [8, 128, 256, 256]   float16 \n",
            "synthesis.b256.conv1   213249      65552    [8, 128, 256, 256]   float16 \n",
            "synthesis.b256.torgb   66051       -        [8, 3, 256, 256]     float16 \n",
            "synthesis.b256:0       -           16       [8, 128, 256, 256]   float16 \n",
            "synthesis.b256:1       -           -        [8, 128, 256, 256]   float32 \n",
            "synthesis.b512.conv0   139457      262160   [8, 64, 512, 512]    float16 \n",
            "synthesis.b512.conv1   69761       262160   [8, 64, 512, 512]    float16 \n",
            "synthesis.b512.torgb   33027       -        [8, 3, 512, 512]     float16 \n",
            "synthesis.b512:0       -           16       [8, 64, 512, 512]    float16 \n",
            "synthesis.b512:1       -           -        [8, 64, 512, 512]    float32 \n",
            "synthesis.b1024.conv0  51297       1048592  [8, 32, 1024, 1024]  float16 \n",
            "synthesis.b1024.conv1  25665       1048592  [8, 32, 1024, 1024]  float16 \n",
            "synthesis.b1024.torgb  16515       -        [8, 3, 1024, 1024]   float16 \n",
            "synthesis.b1024:0      -           16       [8, 32, 1024, 1024]  float16 \n",
            "synthesis.b1024:1      -           -        [8, 32, 1024, 1024]  float32 \n",
            "---                    ---         ---      ---                  ---     \n",
            "Total                  28794124    2797104  -                    -       \n",
            "\n",
            "\n",
            "Discriminator  Parameters  Buffers  Output shape         Datatype\n",
            "---            ---         ---      ---                  ---     \n",
            "b1024.fromrgb  128         16       [8, 32, 1024, 1024]  float16 \n",
            "b1024.skip     2048        16       [8, 64, 512, 512]    float16 \n",
            "b1024.conv0    9248        16       [8, 32, 1024, 1024]  float16 \n",
            "b1024.conv1    18496       16       [8, 64, 512, 512]    float16 \n",
            "b1024          -           16       [8, 64, 512, 512]    float16 \n",
            "b512.skip      8192        16       [8, 128, 256, 256]   float16 \n",
            "b512.conv0     36928       16       [8, 64, 512, 512]    float16 \n",
            "b512.conv1     73856       16       [8, 128, 256, 256]   float16 \n",
            "b512           -           16       [8, 128, 256, 256]   float16 \n",
            "b256.skip      32768       16       [8, 256, 128, 128]   float16 \n",
            "b256.conv0     147584      16       [8, 128, 256, 256]   float16 \n",
            "b256.conv1     295168      16       [8, 256, 128, 128]   float16 \n",
            "b256           -           16       [8, 256, 128, 128]   float16 \n",
            "b128.skip      131072      16       [8, 512, 64, 64]     float16 \n",
            "b128.conv0     590080      16       [8, 256, 128, 128]   float16 \n",
            "b128.conv1     1180160     16       [8, 512, 64, 64]     float16 \n",
            "b128           -           16       [8, 512, 64, 64]     float16 \n",
            "b64.skip       262144      16       [8, 512, 32, 32]     float32 \n",
            "b64.conv0      2359808     16       [8, 512, 64, 64]     float32 \n",
            "b64.conv1      2359808     16       [8, 512, 32, 32]     float32 \n",
            "b64            -           16       [8, 512, 32, 32]     float32 \n",
            "b32.skip       262144      16       [8, 512, 16, 16]     float32 \n",
            "b32.conv0      2359808     16       [8, 512, 32, 32]     float32 \n",
            "b32.conv1      2359808     16       [8, 512, 16, 16]     float32 \n",
            "b32            -           16       [8, 512, 16, 16]     float32 \n",
            "b16.skip       262144      16       [8, 512, 8, 8]       float32 \n",
            "b16.conv0      2359808     16       [8, 512, 16, 16]     float32 \n",
            "b16.conv1      2359808     16       [8, 512, 8, 8]       float32 \n",
            "b16            -           16       [8, 512, 8, 8]       float32 \n",
            "b8.skip        262144      16       [8, 512, 4, 4]       float32 \n",
            "b8.conv0       2359808     16       [8, 512, 8, 8]       float32 \n",
            "b8.conv1       2359808     16       [8, 512, 4, 4]       float32 \n",
            "b8             -           16       [8, 512, 4, 4]       float32 \n",
            "b4.mbstd       -           -        [8, 513, 4, 4]       float32 \n",
            "b4.conv        2364416     16       [8, 512, 4, 4]       float32 \n",
            "b4.fc          4194816     -        [8, 512]             float32 \n",
            "b4.out         513         -        [8, 1]               float32 \n",
            "---            ---         ---      ---                  ---     \n",
            "Total          29012513    544      -                    -       \n",
            "\n",
            "Setting up augmentation...\n",
            "Distributing across 1 GPUs...\n",
            "Setting up training phases...\n",
            "Exporting sample images...\n",
            "Initializing logs...\n",
            "Training for 300 kimg...\n",
            "\n",
            "tick 0     kimg 0.0      time 1m 48s       sec/tick 15.6    sec/kimg 1949.97 maintenance 92.0   cpumem 4.27   gpumem 14.63  augment 0.000\n",
            "Evaluating metrics...\n",
            "{\"results\": {\"fid50k_full\": 319.45569218738916}, \"metric\": \"fid50k_full\", \"total_time\": 2448.7679028511047, \"total_time_str\": \"40m 49s\", \"num_gpus\": 1, \"snapshot_pkl\": \"network-snapshot-000000.pkl\", \"timestamp\": 1645941891.156586}\n",
            "tick 1     kimg 4.0      time 1h 11m 37s   sec/tick 1726.7  sec/kimg 431.67  maintenance 2462.5 cpumem 4.81   gpumem 13.92  augment 0.007\n",
            "tick 2     kimg 8.0      time 1h 40m 25s   sec/tick 1727.8  sec/kimg 431.96  maintenance 0.5    cpumem 4.68   gpumem 13.83  augment 0.014\n",
            "tick 3     kimg 12.0     time 2h 09m 15s   sec/tick 1728.9  sec/kimg 432.22  maintenance 0.5    cpumem 4.35   gpumem 13.84  augment 0.019\n",
            "tick 4     kimg 16.0     time 2h 38m 06s   sec/tick 1730.8  sec/kimg 432.71  maintenance 0.5    cpumem 4.29   gpumem 13.86  augment 0.024\n",
            "tick 5     kimg 20.0     time 3h 06m 56s   sec/tick 1729.9  sec/kimg 432.47  maintenance 0.5    cpumem 4.29   gpumem 13.94  augment 0.028\n",
            "tick 6     kimg 24.0     time 3h 35m 47s   sec/tick 1730.4  sec/kimg 432.59  maintenance 0.5    cpumem 4.29   gpumem 13.91  augment 0.033\n",
            "tick 7     kimg 28.0     time 4h 04m 39s   sec/tick 1731.1  sec/kimg 432.78  maintenance 0.5    cpumem 4.29   gpumem 14.26  augment 0.039\n",
            "tick 8     kimg 32.0     time 4h 33m 32s   sec/tick 1733.1  sec/kimg 433.27  maintenance 0.5    cpumem 4.29   gpumem 13.99  augment 0.042\n",
            "tick 9     kimg 36.0     time 5h 02m 25s   sec/tick 1731.9  sec/kimg 432.99  maintenance 0.5    cpumem 4.29   gpumem 14.11  augment 0.042\n",
            "tick 10    kimg 40.0     time 5h 31m 17s   sec/tick 1732.2  sec/kimg 433.05  maintenance 0.5    cpumem 4.29   gpumem 14.33  augment 0.043\n",
            "Evaluating metrics...\n",
            "{\"results\": {\"fid50k_full\": 227.72379490929558}, \"metric\": \"fid50k_full\", \"total_time\": 2300.0503702163696, \"total_time_str\": \"38m 20s\", \"num_gpus\": 1, \"snapshot_pkl\": \"network-snapshot-000040.pkl\", \"timestamp\": 1645961516.3791277}\n",
            "tick 11    kimg 44.0     time 6h 38m 47s   sec/tick 1731.9  sec/kimg 432.98  maintenance 2318.0 cpumem 5.10   gpumem 13.94  augment 0.041\n",
            "tick 12    kimg 48.0     time 7h 07m 41s   sec/tick 1733.2  sec/kimg 433.29  maintenance 0.5    cpumem 4.83   gpumem 14.38  augment 0.038\n",
            "tick 13    kimg 52.0     time 7h 36m 33s   sec/tick 1731.6  sec/kimg 432.89  maintenance 0.5    cpumem 4.74   gpumem 14.23  augment 0.036\n",
            "tick 14    kimg 56.0     time 8h 05m 25s   sec/tick 1731.3  sec/kimg 432.82  maintenance 0.5    cpumem 4.74   gpumem 13.84  augment 0.033\n",
            "tick 15    kimg 60.0     time 8h 34m 16s   sec/tick 1730.9  sec/kimg 432.73  maintenance 0.5    cpumem 4.74   gpumem 13.99  augment 0.030\n",
            "tick 16    kimg 64.0     time 9h 03m 08s   sec/tick 1731.6  sec/kimg 432.91  maintenance 0.5    cpumem 4.74   gpumem 14.06  augment 0.028\n",
            "tick 17    kimg 68.0     time 9h 31m 59s   sec/tick 1730.3  sec/kimg 432.58  maintenance 0.5    cpumem 4.72   gpumem 13.85  augment 0.026\n",
            "tick 18    kimg 72.0     time 10h 00m 50s  sec/tick 1729.9  sec/kimg 432.48  maintenance 0.5    cpumem 4.72   gpumem 13.99  augment 0.023\n",
            "tick 19    kimg 76.0     time 10h 29m 40s  sec/tick 1729.7  sec/kimg 432.41  maintenance 0.5    cpumem 4.72   gpumem 13.92  augment 0.019\n",
            "tick 20    kimg 80.0     time 10h 58m 31s  sec/tick 1730.6  sec/kimg 432.66  maintenance 0.5    cpumem 4.72   gpumem 14.08  augment 0.016\n",
            "Evaluating metrics...\n",
            "{\"results\": {\"fid50k_full\": 109.92528365767147}, \"metric\": \"fid50k_full\", \"total_time\": 2298.9298639297485, \"total_time_str\": \"38m 19s\", \"num_gpus\": 1, \"snapshot_pkl\": \"network-snapshot-000080.pkl\", \"timestamp\": 1645981147.567224}\n",
            "tick 21    kimg 84.0     time 12h 05m 56s  sec/tick 1729.0  sec/kimg 432.26  maintenance 2315.8 cpumem 5.10   gpumem 13.83  augment 0.014\n",
            "tick 22    kimg 88.0     time 12h 34m 45s  sec/tick 1728.6  sec/kimg 432.15  maintenance 0.5    cpumem 4.55   gpumem 13.83  augment 0.011\n",
            "tick 23    kimg 92.0     time 13h 03m 34s  sec/tick 1728.4  sec/kimg 432.11  maintenance 0.5    cpumem 4.55   gpumem 13.86  augment 0.010\n",
            "tick 24    kimg 96.0     time 13h 32m 24s  sec/tick 1729.7  sec/kimg 432.42  maintenance 0.5    cpumem 4.55   gpumem 13.83  augment 0.009\n",
            "tick 25    kimg 100.0    time 14h 01m 12s  sec/tick 1728.1  sec/kimg 432.01  maintenance 0.5    cpumem 4.55   gpumem 13.83  augment 0.006\n",
            "tick 26    kimg 104.0    time 14h 30m 01s  sec/tick 1727.7  sec/kimg 431.92  maintenance 0.5    cpumem 4.55   gpumem 13.83  augment 0.005\n",
            "tick 27    kimg 108.0    time 14h 58m 49s  sec/tick 1727.6  sec/kimg 431.89  maintenance 0.5    cpumem 4.55   gpumem 13.82  augment 0.003\n",
            "tick 28    kimg 112.0    time 15h 27m 38s  sec/tick 1728.8  sec/kimg 432.19  maintenance 0.5    cpumem 4.55   gpumem 13.90  augment 0.002\n",
            "tick 29    kimg 116.0    time 15h 56m 26s  sec/tick 1727.2  sec/kimg 431.79  maintenance 0.5    cpumem 4.55   gpumem 13.80  augment 0.000\n",
            "tick 30    kimg 120.0    time 16h 25m 13s  sec/tick 1727.1  sec/kimg 431.77  maintenance 0.5    cpumem 4.55   gpumem 13.46  augment 0.000\n",
            "Evaluating metrics...\n",
            "{\"results\": {\"fid50k_full\": 41.468999438590956}, \"metric\": \"fid50k_full\", \"total_time\": 2298.0804278850555, \"total_time_str\": \"38m 18s\", \"num_gpus\": 1, \"snapshot_pkl\": \"network-snapshot-000120.pkl\", \"timestamp\": 1646000749.0556042}\n",
            "tick 31    kimg 124.0    time 17h 32m 35s  sec/tick 1727.1  sec/kimg 431.78  maintenance 2314.9 cpumem 5.33   gpumem 13.83  augment 0.000\n",
            "tick 32    kimg 128.0    time 18h 01m 24s  sec/tick 1728.5  sec/kimg 432.12  maintenance 0.5    cpumem 4.89   gpumem 13.73  augment 0.000\n",
            "tick 33    kimg 132.0    time 18h 30m 12s  sec/tick 1727.0  sec/kimg 431.74  maintenance 0.5    cpumem 4.89   gpumem 13.46  augment 0.000\n",
            "tick 34    kimg 136.0    time 18h 58m 59s  sec/tick 1727.0  sec/kimg 431.75  maintenance 0.5    cpumem 4.89   gpumem 13.57  augment 0.000\n",
            "tick 35    kimg 140.0    time 19h 27m 47s  sec/tick 1727.0  sec/kimg 431.74  maintenance 0.5    cpumem 4.89   gpumem 13.80  augment 0.000\n",
            "tick 36    kimg 144.0    time 19h 56m 36s  sec/tick 1728.5  sec/kimg 432.13  maintenance 0.5    cpumem 4.89   gpumem 13.67  augment 0.000\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Modify these to suit your needs\n",
        "EXPERIMENTS = \"/content/drive/MyDrive/Landscape/high_res_baseline_experiments\"\n",
        "DATA = \"/content/drive/MyDrive/Landscape/seg_train/seg_train/High_res_Processed_mountain\"\n",
        "SNAP = 10\n",
        "init_kimg = 300\n",
        "aug =  'bgcfnc'            #default is bgc\n",
        "batch = 8\n",
        "Useaug = \"ada\"      #default is ada, noaug will not use any augmentations\n",
        "\n",
        "\n",
        "# Build the command and run it, we cannot use augpip argument along with aug\n",
        "# cmd = f\"/usr/bin/python3 /content/stylegan2-ada-pytorch/train.py --snap {SNAP} --batch {batch} --augpipe {aug} --kimg {init_kimg} --outdir {EXPERIMENTS} --data {DATA}\"\n",
        "\n",
        "#for no aug\n",
        "cmd = f\"/usr/bin/python3 /content/stylegan2-ada-pytorch/train.py --snap {SNAP} --batch {batch} --aug {Useaug} --kimg {init_kimg} --outdir {EXPERIMENTS} --data {DATA}\"\n",
        "!{cmd}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xowkI56tjf1K",
        "outputId": "18bf64c9-14da-4f02-ed07-ca5d0c1dfcc9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training options:\n",
            "{\n",
            "  \"num_gpus\": 1,\n",
            "  \"image_snapshot_ticks\": 10,\n",
            "  \"network_snapshot_ticks\": 10,\n",
            "  \"metrics\": [\n",
            "    \"fid50k_full\"\n",
            "  ],\n",
            "  \"random_seed\": 0,\n",
            "  \"training_set_kwargs\": {\n",
            "    \"class_name\": \"training.dataset.ImageFolderDataset\",\n",
            "    \"path\": \"/content/drive/MyDrive/Datasets/Kaggle/Landscape/seg_train/seg_train/High_res_Processed_mountain\",\n",
            "    \"use_labels\": false,\n",
            "    \"max_size\": 2512,\n",
            "    \"xflip\": false,\n",
            "    \"resolution\": 1024\n",
            "  },\n",
            "  \"data_loader_kwargs\": {\n",
            "    \"pin_memory\": true,\n",
            "    \"num_workers\": 3,\n",
            "    \"prefetch_factor\": 2\n",
            "  },\n",
            "  \"G_kwargs\": {\n",
            "    \"class_name\": \"training.networks.Generator\",\n",
            "    \"z_dim\": 512,\n",
            "    \"w_dim\": 512,\n",
            "    \"mapping_kwargs\": {\n",
            "      \"num_layers\": 2\n",
            "    },\n",
            "    \"synthesis_kwargs\": {\n",
            "      \"channel_base\": 32768,\n",
            "      \"channel_max\": 512,\n",
            "      \"num_fp16_res\": 4,\n",
            "      \"conv_clamp\": 256\n",
            "    }\n",
            "  },\n",
            "  \"D_kwargs\": {\n",
            "    \"class_name\": \"training.networks.Discriminator\",\n",
            "    \"block_kwargs\": {},\n",
            "    \"mapping_kwargs\": {},\n",
            "    \"epilogue_kwargs\": {\n",
            "      \"mbstd_group_size\": 4\n",
            "    },\n",
            "    \"channel_base\": 32768,\n",
            "    \"channel_max\": 512,\n",
            "    \"num_fp16_res\": 4,\n",
            "    \"conv_clamp\": 256\n",
            "  },\n",
            "  \"G_opt_kwargs\": {\n",
            "    \"class_name\": \"torch.optim.Adam\",\n",
            "    \"lr\": 0.002,\n",
            "    \"betas\": [\n",
            "      0,\n",
            "      0.99\n",
            "    ],\n",
            "    \"eps\": 1e-08\n",
            "  },\n",
            "  \"D_opt_kwargs\": {\n",
            "    \"class_name\": \"torch.optim.Adam\",\n",
            "    \"lr\": 0.002,\n",
            "    \"betas\": [\n",
            "      0,\n",
            "      0.99\n",
            "    ],\n",
            "    \"eps\": 1e-08\n",
            "  },\n",
            "  \"loss_kwargs\": {\n",
            "    \"class_name\": \"training.loss.StyleGAN2Loss\",\n",
            "    \"r1_gamma\": 52.4288\n",
            "  },\n",
            "  \"total_kimg\": 200,\n",
            "  \"batch_size\": 8,\n",
            "  \"batch_gpu\": 8,\n",
            "  \"ema_kimg\": 1.25,\n",
            "  \"ema_rampup\": null,\n",
            "  \"ada_target\": 0.6,\n",
            "  \"augment_kwargs\": {\n",
            "    \"class_name\": \"training.augment.AugmentPipe\",\n",
            "    \"xflip\": 1,\n",
            "    \"rotate90\": 1,\n",
            "    \"xint\": 1,\n",
            "    \"scale\": 1,\n",
            "    \"rotate\": 1,\n",
            "    \"aniso\": 1,\n",
            "    \"xfrac\": 1,\n",
            "    \"brightness\": 1,\n",
            "    \"contrast\": 1,\n",
            "    \"lumaflip\": 1,\n",
            "    \"hue\": 1,\n",
            "    \"saturation\": 1,\n",
            "    \"imgfilter\": 1,\n",
            "    \"noise\": 1,\n",
            "    \"cutout\": 1\n",
            "  },\n",
            "  \"resume_pkl\": \"/content/drive/MyDrive/Datasets/Kaggle/Landscape/high_res_with_aug_experiments/00007-High_res_Processed_mountain-auto1-kimg200-batch8-bgcfnc-resumecustom/network-snapshot-000040.pkl\",\n",
            "  \"ada_kimg\": 100,\n",
            "  \"run_dir\": \"/content/drive/MyDrive/Datasets/Kaggle/Landscape/high_res_with_aug_experiments/00008-High_res_Processed_mountain-auto1-kimg200-batch8-bgcfnc-resumecustom\"\n",
            "}\n",
            "\n",
            "Output directory:   /content/drive/MyDrive/Datasets/Kaggle/Landscape/high_res_with_aug_experiments/00008-High_res_Processed_mountain-auto1-kimg200-batch8-bgcfnc-resumecustom\n",
            "Training data:      /content/drive/MyDrive/Datasets/Kaggle/Landscape/seg_train/seg_train/High_res_Processed_mountain\n",
            "Training duration:  200 kimg\n",
            "Number of GPUs:     1\n",
            "Number of images:   2512\n",
            "Image resolution:   1024\n",
            "Conditional model:  False\n",
            "Dataset x-flips:    False\n",
            "\n",
            "Creating output directory...\n",
            "Launching processes...\n",
            "Loading training set...\n",
            "\n",
            "Num images:  2512\n",
            "Image shape: [3, 1024, 1024]\n",
            "Label shape: [0]\n",
            "\n",
            "Constructing networks...\n",
            "Resuming from \"/content/drive/MyDrive/Datasets/Kaggle/Landscape/high_res_with_aug_experiments/00007-High_res_Processed_mountain-auto1-kimg200-batch8-bgcfnc-resumecustom/network-snapshot-000040.pkl\"\n",
            "Setting up PyTorch plugin \"bias_act_plugin\"... Done.\n",
            "Setting up PyTorch plugin \"upfirdn2d_plugin\"... Done.\n",
            "\n",
            "Generator              Parameters  Buffers  Output shape         Datatype\n",
            "---                    ---         ---      ---                  ---     \n",
            "mapping.fc0            262656      -        [8, 512]             float32 \n",
            "mapping.fc1            262656      -        [8, 512]             float32 \n",
            "mapping                -           512      [8, 18, 512]         float32 \n",
            "synthesis.b4.conv1     2622465     32       [8, 512, 4, 4]       float32 \n",
            "synthesis.b4.torgb     264195      -        [8, 3, 4, 4]         float32 \n",
            "synthesis.b4:0         8192        16       [8, 512, 4, 4]       float32 \n",
            "synthesis.b4:1         -           -        [8, 512, 4, 4]       float32 \n",
            "synthesis.b8.conv0     2622465     80       [8, 512, 8, 8]       float32 \n",
            "synthesis.b8.conv1     2622465     80       [8, 512, 8, 8]       float32 \n",
            "synthesis.b8.torgb     264195      -        [8, 3, 8, 8]         float32 \n",
            "synthesis.b8:0         -           16       [8, 512, 8, 8]       float32 \n",
            "synthesis.b8:1         -           -        [8, 512, 8, 8]       float32 \n",
            "synthesis.b16.conv0    2622465     272      [8, 512, 16, 16]     float32 \n",
            "synthesis.b16.conv1    2622465     272      [8, 512, 16, 16]     float32 \n",
            "synthesis.b16.torgb    264195      -        [8, 3, 16, 16]       float32 \n",
            "synthesis.b16:0        -           16       [8, 512, 16, 16]     float32 \n",
            "synthesis.b16:1        -           -        [8, 512, 16, 16]     float32 \n",
            "synthesis.b32.conv0    2622465     1040     [8, 512, 32, 32]     float32 \n",
            "synthesis.b32.conv1    2622465     1040     [8, 512, 32, 32]     float32 \n",
            "synthesis.b32.torgb    264195      -        [8, 3, 32, 32]       float32 \n",
            "synthesis.b32:0        -           16       [8, 512, 32, 32]     float32 \n",
            "synthesis.b32:1        -           -        [8, 512, 32, 32]     float32 \n",
            "synthesis.b64.conv0    2622465     4112     [8, 512, 64, 64]     float32 \n",
            "synthesis.b64.conv1    2622465     4112     [8, 512, 64, 64]     float32 \n",
            "synthesis.b64.torgb    264195      -        [8, 3, 64, 64]       float32 \n",
            "synthesis.b64:0        -           16       [8, 512, 64, 64]     float32 \n",
            "synthesis.b64:1        -           -        [8, 512, 64, 64]     float32 \n",
            "synthesis.b128.conv0   1442561     16400    [8, 256, 128, 128]   float16 \n",
            "synthesis.b128.conv1   721409      16400    [8, 256, 128, 128]   float16 \n",
            "synthesis.b128.torgb   132099      -        [8, 3, 128, 128]     float16 \n",
            "synthesis.b128:0       -           16       [8, 256, 128, 128]   float16 \n",
            "synthesis.b128:1       -           -        [8, 256, 128, 128]   float32 \n",
            "synthesis.b256.conv0   426369      65552    [8, 128, 256, 256]   float16 \n",
            "synthesis.b256.conv1   213249      65552    [8, 128, 256, 256]   float16 \n",
            "synthesis.b256.torgb   66051       -        [8, 3, 256, 256]     float16 \n",
            "synthesis.b256:0       -           16       [8, 128, 256, 256]   float16 \n",
            "synthesis.b256:1       -           -        [8, 128, 256, 256]   float32 \n",
            "synthesis.b512.conv0   139457      262160   [8, 64, 512, 512]    float16 \n",
            "synthesis.b512.conv1   69761       262160   [8, 64, 512, 512]    float16 \n",
            "synthesis.b512.torgb   33027       -        [8, 3, 512, 512]     float16 \n",
            "synthesis.b512:0       -           16       [8, 64, 512, 512]    float16 \n",
            "synthesis.b512:1       -           -        [8, 64, 512, 512]    float32 \n",
            "synthesis.b1024.conv0  51297       1048592  [8, 32, 1024, 1024]  float16 \n",
            "synthesis.b1024.conv1  25665       1048592  [8, 32, 1024, 1024]  float16 \n",
            "synthesis.b1024.torgb  16515       -        [8, 3, 1024, 1024]   float16 \n",
            "synthesis.b1024:0      -           16       [8, 32, 1024, 1024]  float16 \n",
            "synthesis.b1024:1      -           -        [8, 32, 1024, 1024]  float32 \n",
            "---                    ---         ---      ---                  ---     \n",
            "Total                  28794124    2797104  -                    -       \n",
            "\n",
            "\n",
            "Discriminator  Parameters  Buffers  Output shape         Datatype\n",
            "---            ---         ---      ---                  ---     \n",
            "b1024.fromrgb  128         16       [8, 32, 1024, 1024]  float16 \n",
            "b1024.skip     2048        16       [8, 64, 512, 512]    float16 \n",
            "b1024.conv0    9248        16       [8, 32, 1024, 1024]  float16 \n",
            "b1024.conv1    18496       16       [8, 64, 512, 512]    float16 \n",
            "b1024          -           16       [8, 64, 512, 512]    float16 \n",
            "b512.skip      8192        16       [8, 128, 256, 256]   float16 \n",
            "b512.conv0     36928       16       [8, 64, 512, 512]    float16 \n",
            "b512.conv1     73856       16       [8, 128, 256, 256]   float16 \n",
            "b512           -           16       [8, 128, 256, 256]   float16 \n",
            "b256.skip      32768       16       [8, 256, 128, 128]   float16 \n",
            "b256.conv0     147584      16       [8, 128, 256, 256]   float16 \n",
            "b256.conv1     295168      16       [8, 256, 128, 128]   float16 \n",
            "b256           -           16       [8, 256, 128, 128]   float16 \n",
            "b128.skip      131072      16       [8, 512, 64, 64]     float16 \n",
            "b128.conv0     590080      16       [8, 256, 128, 128]   float16 \n",
            "b128.conv1     1180160     16       [8, 512, 64, 64]     float16 \n",
            "b128           -           16       [8, 512, 64, 64]     float16 \n",
            "b64.skip       262144      16       [8, 512, 32, 32]     float32 \n",
            "b64.conv0      2359808     16       [8, 512, 64, 64]     float32 \n",
            "b64.conv1      2359808     16       [8, 512, 32, 32]     float32 \n",
            "b64            -           16       [8, 512, 32, 32]     float32 \n",
            "b32.skip       262144      16       [8, 512, 16, 16]     float32 \n",
            "b32.conv0      2359808     16       [8, 512, 32, 32]     float32 \n",
            "b32.conv1      2359808     16       [8, 512, 16, 16]     float32 \n",
            "b32            -           16       [8, 512, 16, 16]     float32 \n",
            "b16.skip       262144      16       [8, 512, 8, 8]       float32 \n",
            "b16.conv0      2359808     16       [8, 512, 16, 16]     float32 \n",
            "b16.conv1      2359808     16       [8, 512, 8, 8]       float32 \n",
            "b16            -           16       [8, 512, 8, 8]       float32 \n",
            "b8.skip        262144      16       [8, 512, 4, 4]       float32 \n",
            "b8.conv0       2359808     16       [8, 512, 8, 8]       float32 \n",
            "b8.conv1       2359808     16       [8, 512, 4, 4]       float32 \n",
            "b8             -           16       [8, 512, 4, 4]       float32 \n",
            "b4.mbstd       -           -        [8, 513, 4, 4]       float32 \n",
            "b4.conv        2364416     16       [8, 512, 4, 4]       float32 \n",
            "b4.fc          4194816     -        [8, 512]             float32 \n",
            "b4.out         513         -        [8, 1]               float32 \n",
            "---            ---         ---      ---                  ---     \n",
            "Total          29012513    544      -                    -       \n",
            "\n",
            "Setting up augmentation...\n",
            "Distributing across 1 GPUs...\n",
            "Setting up training phases...\n",
            "Exporting sample images...\n",
            "Initializing logs...\n",
            "Training for 200 kimg...\n",
            "\n",
            "tick 0     kimg 0.0      time 1m 38s       sec/tick 16.2    sec/kimg 2022.65 maintenance 82.0   cpumem 4.82   gpumem 14.63  augment 0.000\n",
            "Evaluating metrics...\n",
            "{\"results\": {\"fid50k_full\": 49.568958802008524}, \"metric\": \"fid50k_full\", \"total_time\": 2592.2793238162994, \"total_time_str\": \"43m 12s\", \"num_gpus\": 1, \"snapshot_pkl\": \"network-snapshot-000000.pkl\", \"timestamp\": 1636447376.1375291}\n",
            "tick 1     kimg 4.0      time 1h 14m 46s   sec/tick 1778.5  sec/kimg 444.63  maintenance 2608.8 cpumem 5.05   gpumem 14.14  augment 0.000\n",
            "tick 2     kimg 8.0      time 1h 44m 24s   sec/tick 1777.9  sec/kimg 444.48  maintenance 0.5    cpumem 4.56   gpumem 14.14  augment 0.003\n",
            "tick 3     kimg 12.0     time 2h 14m 03s   sec/tick 1778.6  sec/kimg 444.65  maintenance 0.5    cpumem 4.55   gpumem 14.15  augment 0.005\n",
            "tick 4     kimg 16.0     time 2h 43m 43s   sec/tick 1779.6  sec/kimg 444.90  maintenance 0.5    cpumem 4.55   gpumem 14.16  augment 0.002\n",
            "tick 5     kimg 20.0     time 3h 13m 22s   sec/tick 1777.8  sec/kimg 444.46  maintenance 0.5    cpumem 4.55   gpumem 13.96  augment 0.000\n",
            "tick 6     kimg 24.0     time 3h 43m 00s   sec/tick 1778.2  sec/kimg 444.55  maintenance 0.5    cpumem 4.55   gpumem 14.19  augment 0.001\n",
            "tick 7     kimg 28.0     time 4h 12m 39s   sec/tick 1778.6  sec/kimg 444.65  maintenance 0.5    cpumem 4.55   gpumem 14.15  augment 0.004\n",
            "tick 8     kimg 32.0     time 4h 42m 20s   sec/tick 1780.4  sec/kimg 445.10  maintenance 0.5    cpumem 4.55   gpumem 14.15  augment 0.004\n",
            "tick 9     kimg 36.0     time 5h 11m 59s   sec/tick 1778.7  sec/kimg 444.68  maintenance 0.5    cpumem 4.55   gpumem 14.15  augment 0.005\n",
            "tick 10    kimg 40.0     time 5h 41m 39s   sec/tick 1779.2  sec/kimg 444.80  maintenance 0.5    cpumem 4.55   gpumem 14.15  augment 0.013\n",
            "Evaluating metrics...\n",
            "{\"results\": {\"fid50k_full\": 34.29107423543488}, \"metric\": \"fid50k_full\", \"total_time\": 2298.1247322559357, \"total_time_str\": \"38m 18s\", \"num_gpus\": 1, \"snapshot_pkl\": \"network-snapshot-000040.pkl\", \"timestamp\": 1636467483.4807508}\n",
            "tick 11    kimg 44.0     time 6h 49m 54s   sec/tick 1779.8  sec/kimg 444.95  maintenance 2315.3 cpumem 5.55   gpumem 14.43  augment 0.013\n",
            "tick 12    kimg 48.0     time 7h 19m 36s   sec/tick 1781.0  sec/kimg 445.24  maintenance 0.5    cpumem 5.27   gpumem 14.18  augment 0.011\n",
            "tick 13    kimg 52.0     time 7h 49m 15s   sec/tick 1779.2  sec/kimg 444.79  maintenance 0.5    cpumem 5.25   gpumem 14.37  augment 0.012\n",
            "tick 14    kimg 56.0     time 8h 18m 55s   sec/tick 1779.6  sec/kimg 444.89  maintenance 0.5    cpumem 5.23   gpumem 14.32  augment 0.010\n",
            "tick 15    kimg 60.0     time 8h 48m 36s   sec/tick 1779.9  sec/kimg 444.97  maintenance 0.5    cpumem 5.23   gpumem 14.15  augment 0.013\n"
          ]
        }
      ],
      "source": [
        "#for baseline 1024 high res\n",
        "!/usr/bin/python3 /content/stylegan2-ada-pytorch/train.py --kimg 200 --batch 8 --snap 10 --augpipe bgcfnc --resume /content/drive/MyDrive/Landscape/high_res_with_aug_experiments/00007-High_res_Processed_mountain-auto1-kimg200-batch8-bgcfnc-resumecustom/network-snapshot-000040.pkl --outdir /content/drive/MyDrive/Landscape/high_res_with_aug_experiments --data /content/drive/MyDrive/Landscape/seg_train/seg_train/High_res_Processed_mountain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j4g2FUCwG1U_",
        "outputId": "d89d4878-8a42-40a7-bef7-c9ad6a20be5f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training options:\n",
            "{\n",
            "  \"num_gpus\": 1,\n",
            "  \"image_snapshot_ticks\": 10,\n",
            "  \"network_snapshot_ticks\": 10,\n",
            "  \"metrics\": [\n",
            "    \"fid50k_full\"\n",
            "  ],\n",
            "  \"random_seed\": 0,\n",
            "  \"training_set_kwargs\": {\n",
            "    \"class_name\": \"training.dataset.ImageFolderDataset\",\n",
            "    \"path\": \"/content/drive/MyDrive/Datasets/Kaggle/Landscape/seg_train/seg_train/Processed_mountain\",\n",
            "    \"use_labels\": false,\n",
            "    \"max_size\": 2512,\n",
            "    \"xflip\": false,\n",
            "    \"resolution\": 256\n",
            "  },\n",
            "  \"data_loader_kwargs\": {\n",
            "    \"pin_memory\": true,\n",
            "    \"num_workers\": 3,\n",
            "    \"prefetch_factor\": 2\n",
            "  },\n",
            "  \"G_kwargs\": {\n",
            "    \"class_name\": \"training.networks.Generator\",\n",
            "    \"z_dim\": 512,\n",
            "    \"w_dim\": 512,\n",
            "    \"mapping_kwargs\": {\n",
            "      \"num_layers\": 2\n",
            "    },\n",
            "    \"synthesis_kwargs\": {\n",
            "      \"channel_base\": 16384,\n",
            "      \"channel_max\": 512,\n",
            "      \"num_fp16_res\": 4,\n",
            "      \"conv_clamp\": 256\n",
            "    }\n",
            "  },\n",
            "  \"D_kwargs\": {\n",
            "    \"class_name\": \"training.networks.Discriminator\",\n",
            "    \"block_kwargs\": {},\n",
            "    \"mapping_kwargs\": {},\n",
            "    \"epilogue_kwargs\": {\n",
            "      \"mbstd_group_size\": 4\n",
            "    },\n",
            "    \"channel_base\": 16384,\n",
            "    \"channel_max\": 512,\n",
            "    \"num_fp16_res\": 4,\n",
            "    \"conv_clamp\": 256\n",
            "  },\n",
            "  \"G_opt_kwargs\": {\n",
            "    \"class_name\": \"torch.optim.Adam\",\n",
            "    \"lr\": 0.0025,\n",
            "    \"betas\": [\n",
            "      0,\n",
            "      0.99\n",
            "    ],\n",
            "    \"eps\": 1e-08\n",
            "  },\n",
            "  \"D_opt_kwargs\": {\n",
            "    \"class_name\": \"torch.optim.Adam\",\n",
            "    \"lr\": 0.0025,\n",
            "    \"betas\": [\n",
            "      0,\n",
            "      0.99\n",
            "    ],\n",
            "    \"eps\": 1e-08\n",
            "  },\n",
            "  \"loss_kwargs\": {\n",
            "    \"class_name\": \"training.loss.StyleGAN2Loss\",\n",
            "    \"r1_gamma\": 0.8192\n",
            "  },\n",
            "  \"total_kimg\": 200,\n",
            "  \"batch_size\": 48,\n",
            "  \"batch_gpu\": 48,\n",
            "  \"ema_kimg\": 5.0,\n",
            "  \"ema_rampup\": null,\n",
            "  \"ada_target\": 0.6,\n",
            "  \"augment_kwargs\": {\n",
            "    \"class_name\": \"training.augment.AugmentPipe\",\n",
            "    \"xflip\": 1,\n",
            "    \"rotate90\": 1,\n",
            "    \"xint\": 1,\n",
            "    \"scale\": 1,\n",
            "    \"rotate\": 1,\n",
            "    \"aniso\": 1,\n",
            "    \"xfrac\": 1,\n",
            "    \"brightness\": 1,\n",
            "    \"contrast\": 1,\n",
            "    \"lumaflip\": 1,\n",
            "    \"hue\": 1,\n",
            "    \"saturation\": 1,\n",
            "    \"imgfilter\": 1,\n",
            "    \"noise\": 1,\n",
            "    \"cutout\": 1\n",
            "  },\n",
            "  \"resume_pkl\": \"/content/drive/MyDrive/Datasets/Kaggle/Landscape/modified_gen_with_aug_experiments/00000-Processed_mountain-auto1-kimg300-batch48-bgcfnc/network-snapshot-000121.pkl\",\n",
            "  \"ada_kimg\": 100,\n",
            "  \"run_dir\": \"/content/drive/MyDrive/Datasets/Kaggle/Landscape/modified_gen_with_aug_experiments/00001-Processed_mountain-auto1-kimg200-batch48-bgcfnc-resumecustom\"\n",
            "}\n",
            "\n",
            "Output directory:   /content/drive/MyDrive/Datasets/Kaggle/Landscape/modified_gen_with_aug_experiments/00001-Processed_mountain-auto1-kimg200-batch48-bgcfnc-resumecustom\n",
            "Training data:      /content/drive/MyDrive/Datasets/Kaggle/Landscape/seg_train/seg_train/Processed_mountain\n",
            "Training duration:  200 kimg\n",
            "Number of GPUs:     1\n",
            "Number of images:   2512\n",
            "Image resolution:   256\n",
            "Conditional model:  False\n",
            "Dataset x-flips:    False\n",
            "\n",
            "Creating output directory...\n",
            "Launching processes...\n",
            "Loading training set...\n",
            "\n",
            "Num images:  2512\n",
            "Image shape: [3, 256, 256]\n",
            "Label shape: [0]\n",
            "\n",
            "Constructing networks...\n",
            "Resuming from \"/content/drive/MyDrive/Datasets/Kaggle/Landscape/modified_gen_with_aug_experiments/00000-Processed_mountain-auto1-kimg300-batch48-bgcfnc/network-snapshot-000121.pkl\"\n",
            "Setting up PyTorch plugin \"bias_act_plugin\"... Done.\n",
            "Setting up PyTorch plugin \"upfirdn2d_plugin\"... Done.\n",
            "\n",
            "Generator             Parameters  Buffers  Output shape         Datatype\n",
            "---                   ---         ---      ---                  ---     \n",
            "mapping.fc0           262656      -        [48, 512]            float32 \n",
            "mapping.fc1           262656      -        [48, 512]            float32 \n",
            "mapping               -           512      [48, 14, 512]        float32 \n",
            "synthesis.b4.conv1    2622465     32       [48, 512, 4, 4]      float32 \n",
            "synthesis.b4          8192        16       [48, 512, 4, 4]      float32 \n",
            "synthesis.b8.skip     262144      16       [48, 512, 8, 8]      float32 \n",
            "synthesis.b8.conv0    2622465     80       [48, 512, 8, 8]      float32 \n",
            "synthesis.b8.conv1    2622465     80       [48, 512, 8, 8]      float32 \n",
            "synthesis.b8          -           16       [48, 512, 8, 8]      float32 \n",
            "synthesis.b16.skip    262144      16       [48, 512, 16, 16]    float32 \n",
            "synthesis.b16.conv0   2622465     272      [48, 512, 16, 16]    float32 \n",
            "synthesis.b16.conv1   2622465     272      [48, 512, 16, 16]    float32 \n",
            "synthesis.b16         -           16       [48, 512, 16, 16]    float32 \n",
            "synthesis.b32.skip    262144      16       [48, 512, 32, 32]    float16 \n",
            "synthesis.b32.conv0   2622465     1040     [48, 512, 32, 32]    float16 \n",
            "synthesis.b32.conv1   2622465     1040     [48, 512, 32, 32]    float16 \n",
            "synthesis.b32         -           16       [48, 512, 32, 32]    float16 \n",
            "synthesis.b64.skip    131072      16       [48, 256, 64, 64]    float16 \n",
            "synthesis.b64.conv0   1442561     4112     [48, 256, 64, 64]    float16 \n",
            "synthesis.b64.conv1   721409      4112     [48, 256, 64, 64]    float16 \n",
            "synthesis.b64         -           16       [48, 256, 64, 64]    float16 \n",
            "synthesis.b128.skip   32768       16       [48, 128, 128, 128]  float16 \n",
            "synthesis.b128.conv0  426369      16400    [48, 128, 128, 128]  float16 \n",
            "synthesis.b128.conv1  213249      16400    [48, 128, 128, 128]  float16 \n",
            "synthesis.b128        -           16       [48, 128, 128, 128]  float16 \n",
            "synthesis.b256.skip   8192        16       [48, 64, 256, 256]   float16 \n",
            "synthesis.b256.conv0  139457      65552    [48, 64, 256, 256]   float16 \n",
            "synthesis.b256.conv1  69761       65552    [48, 64, 256, 256]   float16 \n",
            "synthesis.b256.torgb  33027       -        [48, 3, 256, 256]    float16 \n",
            "synthesis.b256:0      -           16       [48, 64, 256, 256]   float16 \n",
            "synthesis.b256:1      -           -        [48, 64, 256, 256]   float32 \n",
            "---                   ---         ---      ---                  ---     \n",
            "Total                 22895056    175664   -                    -       \n",
            "\n",
            "\n",
            "Discriminator  Parameters  Buffers  Output shape         Datatype\n",
            "---            ---         ---      ---                  ---     \n",
            "b256.fromrgb   256         16       [48, 64, 256, 256]   float16 \n",
            "b256.skip      8192        16       [48, 128, 128, 128]  float16 \n",
            "b256.conv0     36928       16       [48, 64, 256, 256]   float16 \n",
            "b256.conv1     73856       16       [48, 128, 128, 128]  float16 \n",
            "b256           -           16       [48, 128, 128, 128]  float16 \n",
            "b128.skip      32768       16       [48, 256, 64, 64]    float16 \n",
            "b128.conv0     147584      16       [48, 128, 128, 128]  float16 \n",
            "b128.conv1     295168      16       [48, 256, 64, 64]    float16 \n",
            "b128           -           16       [48, 256, 64, 64]    float16 \n",
            "b64.skip       131072      16       [48, 512, 32, 32]    float16 \n",
            "b64.conv0      590080      16       [48, 256, 64, 64]    float16 \n",
            "b64.conv1      1180160     16       [48, 512, 32, 32]    float16 \n",
            "b64            -           16       [48, 512, 32, 32]    float16 \n",
            "b32.skip       262144      16       [48, 512, 16, 16]    float16 \n",
            "b32.conv0      2359808     16       [48, 512, 32, 32]    float16 \n",
            "b32.conv1      2359808     16       [48, 512, 16, 16]    float16 \n",
            "b32            -           16       [48, 512, 16, 16]    float16 \n",
            "b16.skip       262144      16       [48, 512, 8, 8]      float32 \n",
            "b16.conv0      2359808     16       [48, 512, 16, 16]    float32 \n",
            "b16.conv1      2359808     16       [48, 512, 8, 8]      float32 \n",
            "b16            -           16       [48, 512, 8, 8]      float32 \n",
            "b8.skip        262144      16       [48, 512, 4, 4]      float32 \n",
            "b8.conv0       2359808     16       [48, 512, 8, 8]      float32 \n",
            "b8.conv1       2359808     16       [48, 512, 4, 4]      float32 \n",
            "b8             -           16       [48, 512, 4, 4]      float32 \n",
            "b4.mbstd       -           -        [48, 513, 4, 4]      float32 \n",
            "b4.conv        2364416     16       [48, 512, 4, 4]      float32 \n",
            "b4.fc          4194816     -        [48, 512]            float32 \n",
            "b4.out         513         -        [48, 1]              float32 \n",
            "---            ---         ---      ---                  ---     \n",
            "Total          24001089    416      -                    -       \n",
            "\n",
            "Setting up augmentation...\n",
            "Distributing across 1 GPUs...\n",
            "Setting up training phases...\n",
            "Exporting sample images...\n",
            "Initializing logs...\n",
            "Training for 200 kimg...\n",
            "\n",
            "tick 0     kimg 0.0      time 53s          sec/tick 10.3    sec/kimg 214.87  maintenance 42.2   cpumem 4.67   gpumem 13.46  augment 0.000\n",
            "Evaluating metrics...\n",
            "{\"results\": {\"fid50k_full\": 278.8362548537321}, \"metric\": \"fid50k_full\", \"total_time\": 616.277524471283, \"total_time_str\": \"10m 16s\", \"num_gpus\": 1, \"snapshot_pkl\": \"network-snapshot-000000.pkl\", \"timestamp\": 1635217729.5986874}\n",
            "tick 1     kimg 4.1      time 15m 46s      sec/tick 259.8   sec/kimg 64.44   maintenance 633.1  cpumem 4.75   gpumem 13.69  augment 0.000\n",
            "tick 2     kimg 8.1      time 20m 07s      sec/tick 261.0   sec/kimg 64.73   maintenance 0.5    cpumem 4.75   gpumem 13.28  augment 0.000\n",
            "tick 3     kimg 12.1     time 24m 29s      sec/tick 261.1   sec/kimg 64.77   maintenance 0.5    cpumem 4.75   gpumem 13.31  augment 0.000\n",
            "tick 4     kimg 16.2     time 28m 51s      sec/tick 262.1   sec/kimg 65.01   maintenance 0.5    cpumem 4.75   gpumem 13.26  augment 0.000\n",
            "tick 5     kimg 20.2     time 33m 13s      sec/tick 261.1   sec/kimg 64.76   maintenance 0.7    cpumem 4.75   gpumem 13.25  augment 0.000\n",
            "tick 6     kimg 24.2     time 37m 35s      sec/tick 261.1   sec/kimg 64.75   maintenance 0.5    cpumem 4.75   gpumem 13.24  augment 0.000\n",
            "tick 7     kimg 28.3     time 41m 56s      sec/tick 261.1   sec/kimg 64.76   maintenance 0.5    cpumem 4.75   gpumem 13.25  augment 0.000\n",
            "tick 8     kimg 32.3     time 46m 19s      sec/tick 262.1   sec/kimg 65.01   maintenance 0.5    cpumem 4.75   gpumem 13.30  augment 0.000\n",
            "tick 9     kimg 36.3     time 50m 41s      sec/tick 261.1   sec/kimg 64.76   maintenance 0.7    cpumem 4.75   gpumem 13.36  augment 0.000\n",
            "tick 10    kimg 40.4     time 55m 03s      sec/tick 261.1   sec/kimg 64.76   maintenance 0.5    cpumem 4.75   gpumem 13.35  augment 0.002\n",
            "Evaluating metrics...\n",
            "{\"results\": {\"fid50k_full\": 202.32174167513932}, \"metric\": \"fid50k_full\", \"total_time\": 616.5378305912018, \"total_time_str\": \"10m 17s\", \"num_gpus\": 1, \"snapshot_pkl\": \"network-snapshot-000040.pkl\", \"timestamp\": 1635220981.9364018}\n",
            "tick 11    kimg 44.4     time 1h 09m 59s   sec/tick 261.3   sec/kimg 64.81   maintenance 635.4  cpumem 5.45   gpumem 13.36  augment 0.002\n",
            "tick 12    kimg 48.4     time 1h 14m 22s   sec/tick 262.2   sec/kimg 65.03   maintenance 0.5    cpumem 5.45   gpumem 13.36  augment 0.002\n",
            "tick 13    kimg 52.5     time 1h 18m 44s   sec/tick 261.2   sec/kimg 64.78   maintenance 0.7    cpumem 5.45   gpumem 13.33  augment 0.000\n",
            "tick 14    kimg 56.5     time 1h 23m 06s   sec/tick 261.2   sec/kimg 64.79   maintenance 0.5    cpumem 5.45   gpumem 13.36  augment 0.000\n",
            "tick 15    kimg 60.5     time 1h 27m 28s   sec/tick 261.3   sec/kimg 64.80   maintenance 0.5    cpumem 5.45   gpumem 13.36  augment 0.000\n",
            "tick 16    kimg 64.6     time 1h 31m 50s   sec/tick 262.3   sec/kimg 65.05   maintenance 0.5    cpumem 5.45   gpumem 13.36  augment 0.004\n",
            "tick 17    kimg 68.6     time 1h 36m 13s   sec/tick 261.4   sec/kimg 64.82   maintenance 0.7    cpumem 5.45   gpumem 13.36  augment 0.000\n",
            "tick 18    kimg 72.6     time 1h 40m 34s   sec/tick 261.3   sec/kimg 64.82   maintenance 0.5    cpumem 5.45   gpumem 13.33  augment 0.000\n",
            "tick 19    kimg 76.7     time 1h 44m 56s   sec/tick 261.3   sec/kimg 64.80   maintenance 0.5    cpumem 5.45   gpumem 13.35  augment 0.000\n",
            "tick 20    kimg 80.7     time 1h 49m 19s   sec/tick 262.3   sec/kimg 65.07   maintenance 0.5    cpumem 5.45   gpumem 13.35  augment 0.000\n",
            "Evaluating metrics...\n",
            "{\"results\": {\"fid50k_full\": 101.86278082895397}, \"metric\": \"fid50k_full\", \"total_time\": 618.9648551940918, \"total_time_str\": \"10m 19s\", \"num_gpus\": 1, \"snapshot_pkl\": \"network-snapshot-000080.pkl\", \"timestamp\": 1635224240.8815193}\n",
            "tick 21    kimg 84.7     time 2h 04m 18s   sec/tick 261.3   sec/kimg 64.81   maintenance 637.8  cpumem 5.45   gpumem 13.36  augment 0.004\n",
            "tick 22    kimg 88.8     time 2h 08m 40s   sec/tick 261.3   sec/kimg 64.82   maintenance 0.5    cpumem 5.45   gpumem 13.36  augment 0.002\n",
            "tick 23    kimg 92.8     time 2h 13m 02s   sec/tick 261.3   sec/kimg 64.82   maintenance 0.5    cpumem 5.45   gpumem 13.36  augment 0.002\n",
            "tick 24    kimg 96.8     time 2h 17m 25s   sec/tick 262.3   sec/kimg 65.05   maintenance 0.5    cpumem 5.45   gpumem 13.34  augment 0.002\n",
            "tick 25    kimg 100.8    time 2h 21m 47s   sec/tick 261.3   sec/kimg 64.80   maintenance 0.7    cpumem 5.45   gpumem 13.37  augment 0.000\n",
            "tick 26    kimg 104.9    time 2h 26m 09s   sec/tick 261.4   sec/kimg 64.83   maintenance 0.5    cpumem 5.45   gpumem 13.36  augment 0.002\n",
            "tick 27    kimg 108.9    time 2h 30m 31s   sec/tick 261.3   sec/kimg 64.80   maintenance 0.5    cpumem 5.45   gpumem 13.36  augment 0.000\n",
            "tick 28    kimg 112.9    time 2h 34m 53s   sec/tick 262.3   sec/kimg 65.06   maintenance 0.5    cpumem 5.45   gpumem 13.35  augment 0.000\n",
            "tick 29    kimg 117.0    time 2h 39m 16s   sec/tick 261.4   sec/kimg 64.82   maintenance 0.7    cpumem 5.45   gpumem 13.36  augment 0.004\n",
            "tick 30    kimg 121.0    time 2h 43m 38s   sec/tick 261.6   sec/kimg 64.87   maintenance 0.5    cpumem 5.45   gpumem 13.37  augment 0.013\n",
            "Evaluating metrics...\n",
            "{\"results\": {\"fid50k_full\": 92.84051518671967}, \"metric\": \"fid50k_full\", \"total_time\": 615.8456678390503, \"total_time_str\": \"10m 16s\", \"num_gpus\": 1, \"snapshot_pkl\": \"network-snapshot-000121.pkl\", \"timestamp\": 1635227495.0889177}\n",
            "tick 31    kimg 125.0    time 2h 58m 33s   sec/tick 261.6   sec/kimg 64.89   maintenance 633.5  cpumem 5.45   gpumem 13.37  augment 0.008\n",
            "tick 32    kimg 129.1    time 3h 02m 56s   sec/tick 262.7   sec/kimg 65.15   maintenance 0.5    cpumem 5.45   gpumem 13.42  augment 0.017\n",
            "tick 33    kimg 133.1    time 3h 07m 18s   sec/tick 261.6   sec/kimg 64.89   maintenance 0.7    cpumem 5.45   gpumem 13.39  augment 0.010\n",
            "tick 34    kimg 137.1    time 3h 11m 40s   sec/tick 261.4   sec/kimg 64.83   maintenance 0.5    cpumem 5.45   gpumem 13.38  augment 0.012\n",
            "tick 35    kimg 141.2    time 3h 16m 02s   sec/tick 261.4   sec/kimg 64.84   maintenance 0.5    cpumem 5.45   gpumem 13.39  augment 0.010\n",
            "tick 36    kimg 145.2    time 3h 20m 25s   sec/tick 262.5   sec/kimg 65.11   maintenance 0.5    cpumem 5.45   gpumem 13.52  augment 0.019\n",
            "tick 37    kimg 149.2    time 3h 24m 48s   sec/tick 261.7   sec/kimg 64.89   maintenance 0.7    cpumem 5.45   gpumem 13.40  augment 0.017\n",
            "tick 38    kimg 153.3    time 3h 29m 10s   sec/tick 261.6   sec/kimg 64.89   maintenance 0.5    cpumem 5.45   gpumem 13.42  augment 0.012\n",
            "tick 39    kimg 157.3    time 3h 33m 32s   sec/tick 261.5   sec/kimg 64.86   maintenance 0.5    cpumem 5.45   gpumem 13.38  augment 0.010\n",
            "tick 40    kimg 161.3    time 3h 37m 55s   sec/tick 262.7   sec/kimg 65.14   maintenance 0.5    cpumem 5.45   gpumem 13.41  augment 0.023\n",
            "Evaluating metrics...\n",
            "{\"results\": {\"fid50k_full\": 92.71134823371456}, \"metric\": \"fid50k_full\", \"total_time\": 614.3049349784851, \"total_time_str\": \"10m 14s\", \"num_gpus\": 1, \"snapshot_pkl\": \"network-snapshot-000161.pkl\", \"timestamp\": 1635230751.040243}\n",
            "tick 41    kimg 165.4    time 3h 52m 49s   sec/tick 261.9   sec/kimg 64.95   maintenance 631.9  cpumem 5.45   gpumem 13.41  augment 0.025\n",
            "tick 42    kimg 169.4    time 3h 57m 11s   sec/tick 261.7   sec/kimg 64.91   maintenance 0.5    cpumem 5.45   gpumem 13.39  augment 0.023\n",
            "tick 43    kimg 173.4    time 4h 01m 34s   sec/tick 261.8   sec/kimg 64.92   maintenance 0.5    cpumem 5.45   gpumem 13.41  augment 0.036\n",
            "tick 44    kimg 177.5    time 4h 05m 57s   sec/tick 262.8   sec/kimg 65.18   maintenance 0.5    cpumem 5.45   gpumem 13.40  augment 0.027\n",
            "tick 45    kimg 181.5    time 4h 10m 19s   sec/tick 261.7   sec/kimg 64.90   maintenance 0.7    cpumem 5.45   gpumem 13.41  augment 0.029\n",
            "tick 46    kimg 185.5    time 4h 14m 42s   sec/tick 261.9   sec/kimg 64.97   maintenance 0.5    cpumem 5.45   gpumem 13.43  augment 0.029\n",
            "tick 47    kimg 189.6    time 4h 19m 04s   sec/tick 262.0   sec/kimg 64.99   maintenance 0.5    cpumem 5.45   gpumem 13.45  augment 0.027\n",
            "tick 48    kimg 193.6    time 4h 23m 28s   sec/tick 262.9   sec/kimg 65.21   maintenance 0.5    cpumem 5.45   gpumem 13.42  augment 0.017\n",
            "tick 49    kimg 197.6    time 4h 27m 50s   sec/tick 261.9   sec/kimg 64.95   maintenance 0.7    cpumem 5.45   gpumem 13.39  augment 0.023\n",
            "tick 50    kimg 200.0    time 4h 30m 26s   sec/tick 155.2   sec/kimg 64.68   maintenance 0.5    cpumem 5.45   gpumem 13.39  augment 0.035\n",
            "Evaluating metrics...\n",
            "{\"results\": {\"fid50k_full\": 72.63641502500582}, \"metric\": \"fid50k_full\", \"total_time\": 616.6584351062775, \"total_time_str\": \"10m 17s\", \"num_gpus\": 1, \"snapshot_pkl\": \"network-snapshot-000200.pkl\", \"timestamp\": 1635233905.065039}\n",
            "\n",
            "Exiting...\n"
          ]
        }
      ],
      "source": [
        "#for previous modified gen, 256 size\n",
        "!/usr/bin/python3 /content/stylegan2-ada-pytorch/train.py --kimg 200 --batch 48 --snap 10 --augpipe bgcfnc --resume /content/drive/MyDrive/Datasets/Kaggle/Landscape/modified_gen_with_aug_experiments/00000-Processed_mountain-auto1-kimg300-batch48-bgcfnc/network-snapshot-000121.pkl --outdir /content/drive/MyDrive/Datasets/Kaggle/Landscape/modified_gen_with_aug_experiments --data /content/drive/MyDrive/Datasets/Kaggle/Landscape/seg_train/seg_train/Processed_mountain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NS-oe6jMG_0A"
      },
      "source": [
        "# Resume Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YyHA_NGTzYsI"
      },
      "outputs": [],
      "source": [
        "!/usr/bin/python3 /content/stylegan2-ada-pytorch/train.py --kimg 1000 --batch 48 --snap 10 --resume /content/drive/MyDrive/Datasets/Kaggle/Landscape/experiments/00002-Processed_mountain-auto1-kimg1000-batch20-resumecustom/network-snapshot-000100.pkl --outdir /content/drive/MyDrive/Datasets/Kaggle/Landscape/experiments --data /content/drive/MyDrive/Datasets/Kaggle/Landscape/seg_train/seg_train/Processed_mountain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AW7ixKkHBB4D"
      },
      "source": [
        "## Generate Images using saved generator model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NBL3h-8JBA6u",
        "outputId": "be568aaf-a4f0-4d8c-a4ed-78b80104f4df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading networks from \"/content/drive/MyDrive/Datasets/Kaggle/Landscape/experiments_baseline/00003-Processed_mountain-auto1-kimg1000-batch48-resumecustom/network-snapshot-000282.pkl\"...\n",
            "Generating image for seed 100 (0/6) ...\n",
            "Setting up PyTorch plugin \"bias_act_plugin\"... Done.\n",
            "Setting up PyTorch plugin \"upfirdn2d_plugin\"... Done.\n",
            "Generating image for seed 101 (1/6) ...\n",
            "Generating image for seed 102 (2/6) ...\n",
            "Generating image for seed 103 (3/6) ...\n",
            "Generating image for seed 104 (4/6) ...\n",
            "Generating image for seed 105 (5/6) ...\n"
          ]
        }
      ],
      "source": [
        "!python /content/stylegan2-ada-pytorch/generate.py --outdir=out --seeds=100-105 \\\n",
        "    --network=/content/drive/MyDrive/Datasets/Kaggle/Landscape/experiments_baseline/00003-Processed_mountain-auto1-kimg1000-batch48-resumecustom/network-snapshot-000282.pkl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pvPZLsCUlzGn",
        "outputId": "803ae60b-2cf3-4e5d-d008-5a991f0e55cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training options:\n",
            "{\n",
            "  \"num_gpus\": 1,\n",
            "  \"image_snapshot_ticks\": 10,\n",
            "  \"network_snapshot_ticks\": 10,\n",
            "  \"metrics\": [\n",
            "    \"fid50k_full\"\n",
            "  ],\n",
            "  \"random_seed\": 0,\n",
            "  \"training_set_kwargs\": {\n",
            "    \"class_name\": \"training.dataset.ImageFolderDataset\",\n",
            "    \"path\": \"/content/drive/MyDrive/Datasets/Kaggle/Landscape/seg_train/seg_train/Processed_mountain\",\n",
            "    \"use_labels\": false,\n",
            "    \"max_size\": 2512,\n",
            "    \"xflip\": false,\n",
            "    \"resolution\": 256\n",
            "  },\n",
            "  \"data_loader_kwargs\": {\n",
            "    \"pin_memory\": true,\n",
            "    \"num_workers\": 3,\n",
            "    \"prefetch_factor\": 2\n",
            "  },\n",
            "  \"G_kwargs\": {\n",
            "    \"class_name\": \"training.networks.Generator\",\n",
            "    \"z_dim\": 512,\n",
            "    \"w_dim\": 512,\n",
            "    \"mapping_kwargs\": {\n",
            "      \"num_layers\": 2\n",
            "    },\n",
            "    \"synthesis_kwargs\": {\n",
            "      \"channel_base\": 16384,\n",
            "      \"channel_max\": 512,\n",
            "      \"num_fp16_res\": 4,\n",
            "      \"conv_clamp\": 256\n",
            "    }\n",
            "  },\n",
            "  \"D_kwargs\": {\n",
            "    \"class_name\": \"training.networks.Discriminator\",\n",
            "    \"block_kwargs\": {},\n",
            "    \"mapping_kwargs\": {},\n",
            "    \"epilogue_kwargs\": {\n",
            "      \"mbstd_group_size\": 4\n",
            "    },\n",
            "    \"channel_base\": 16384,\n",
            "    \"channel_max\": 512,\n",
            "    \"num_fp16_res\": 4,\n",
            "    \"conv_clamp\": 256\n",
            "  },\n",
            "  \"G_opt_kwargs\": {\n",
            "    \"class_name\": \"torch.optim.Adam\",\n",
            "    \"lr\": 0.0025,\n",
            "    \"betas\": [\n",
            "      0,\n",
            "      0.99\n",
            "    ],\n",
            "    \"eps\": 1e-08\n",
            "  },\n",
            "  \"D_opt_kwargs\": {\n",
            "    \"class_name\": \"torch.optim.Adam\",\n",
            "    \"lr\": 0.0025,\n",
            "    \"betas\": [\n",
            "      0,\n",
            "      0.99\n",
            "    ],\n",
            "    \"eps\": 1e-08\n",
            "  },\n",
            "  \"loss_kwargs\": {\n",
            "    \"class_name\": \"training.loss.StyleGAN2Loss\",\n",
            "    \"r1_gamma\": 0.8192\n",
            "  },\n",
            "  \"total_kimg\": 25000,\n",
            "  \"batch_size\": 16,\n",
            "  \"batch_gpu\": 16,\n",
            "  \"ema_kimg\": 5.0,\n",
            "  \"ema_rampup\": null,\n",
            "  \"ada_target\": 0.6,\n",
            "  \"augment_kwargs\": {\n",
            "    \"class_name\": \"training.augment.AugmentPipe\",\n",
            "    \"xflip\": 1,\n",
            "    \"rotate90\": 1,\n",
            "    \"xint\": 1,\n",
            "    \"scale\": 1,\n",
            "    \"rotate\": 1,\n",
            "    \"aniso\": 1,\n",
            "    \"xfrac\": 1,\n",
            "    \"brightness\": 1,\n",
            "    \"contrast\": 1,\n",
            "    \"lumaflip\": 1,\n",
            "    \"hue\": 1,\n",
            "    \"saturation\": 1\n",
            "  },\n",
            "  \"resume_pkl\": \"/content/drive/MyDrive/Datasets/Kaggle/Landscape/experiments/00001-Processed_mountain-auto1-resumecustom/network-snapshot-000000.pkl\",\n",
            "  \"ada_kimg\": 100,\n",
            "  \"run_dir\": \"/content/drive/MyDrive/Datasets/Kaggle/Landscape/experiments/00002-Processed_mountain-auto1-resumecustom\"\n",
            "}\n",
            "\n",
            "Output directory:   /content/drive/MyDrive/Datasets/Kaggle/Landscape/experiments/00002-Processed_mountain-auto1-resumecustom\n",
            "Training data:      /content/drive/MyDrive/Datasets/Kaggle/Landscape/seg_train/seg_train/Processed_mountain\n",
            "Training duration:  25000 kimg\n",
            "Number of GPUs:     1\n",
            "Number of images:   2512\n",
            "Image resolution:   256\n",
            "Conditional model:  False\n",
            "Dataset x-flips:    False\n",
            "\n",
            "Creating output directory...\n",
            "Launching processes...\n",
            "Loading training set...\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 3 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "Num images:  2512\n",
            "Image shape: [3, 256, 256]\n",
            "Label shape: [0]\n",
            "\n",
            "Constructing networks...\n",
            "Resuming from \"/content/drive/MyDrive/Datasets/Kaggle/Landscape/experiments/00001-Processed_mountain-auto1-resumecustom/network-snapshot-000000.pkl\"\n",
            "Setting up PyTorch plugin \"bias_act_plugin\"... Done.\n",
            "Setting up PyTorch plugin \"upfirdn2d_plugin\"... Done.\n",
            "\n",
            "Generator             Parameters  Buffers  Output shape         Datatype\n",
            "---                   ---         ---      ---                  ---     \n",
            "mapping.fc0           262656      -        [16, 512]            float32 \n",
            "mapping.fc1           262656      -        [16, 512]            float32 \n",
            "mapping               -           512      [16, 14, 512]        float32 \n",
            "synthesis.b4.conv1    2622465     32       [16, 512, 4, 4]      float32 \n",
            "synthesis.b4.torgb    264195      -        [16, 3, 4, 4]        float32 \n",
            "synthesis.b4:0        8192        16       [16, 512, 4, 4]      float32 \n",
            "synthesis.b4:1        -           -        [16, 512, 4, 4]      float32 \n",
            "synthesis.b8.conv0    2622465     80       [16, 512, 8, 8]      float32 \n",
            "synthesis.b8.conv1    2622465     80       [16, 512, 8, 8]      float32 \n",
            "synthesis.b8.torgb    264195      -        [16, 3, 8, 8]        float32 \n",
            "synthesis.b8:0        -           16       [16, 512, 8, 8]      float32 \n",
            "synthesis.b8:1        -           -        [16, 512, 8, 8]      float32 \n",
            "synthesis.b16.conv0   2622465     272      [16, 512, 16, 16]    float32 \n",
            "synthesis.b16.conv1   2622465     272      [16, 512, 16, 16]    float32 \n",
            "synthesis.b16.torgb   264195      -        [16, 3, 16, 16]      float32 \n",
            "synthesis.b16:0       -           16       [16, 512, 16, 16]    float32 \n",
            "synthesis.b16:1       -           -        [16, 512, 16, 16]    float32 \n",
            "synthesis.b32.conv0   2622465     1040     [16, 512, 32, 32]    float16 \n",
            "synthesis.b32.conv1   2622465     1040     [16, 512, 32, 32]    float16 \n",
            "synthesis.b32.torgb   264195      -        [16, 3, 32, 32]      float16 \n",
            "synthesis.b32:0       -           16       [16, 512, 32, 32]    float16 \n",
            "synthesis.b32:1       -           -        [16, 512, 32, 32]    float32 \n",
            "synthesis.b64.conv0   1442561     4112     [16, 256, 64, 64]    float16 \n",
            "synthesis.b64.conv1   721409      4112     [16, 256, 64, 64]    float16 \n",
            "synthesis.b64.torgb   132099      -        [16, 3, 64, 64]      float16 \n",
            "synthesis.b64:0       -           16       [16, 256, 64, 64]    float16 \n",
            "synthesis.b64:1       -           -        [16, 256, 64, 64]    float32 \n",
            "synthesis.b128.conv0  426369      16400    [16, 128, 128, 128]  float16 \n",
            "synthesis.b128.conv1  213249      16400    [16, 128, 128, 128]  float16 \n",
            "synthesis.b128.torgb  66051       -        [16, 3, 128, 128]    float16 \n",
            "synthesis.b128:0      -           16       [16, 128, 128, 128]  float16 \n",
            "synthesis.b128:1      -           -        [16, 128, 128, 128]  float32 \n",
            "synthesis.b256.conv0  139457      65552    [16, 64, 256, 256]   float16 \n",
            "synthesis.b256.conv1  69761       65552    [16, 64, 256, 256]   float16 \n",
            "synthesis.b256.torgb  33027       -        [16, 3, 256, 256]    float16 \n",
            "synthesis.b256:0      -           16       [16, 64, 256, 256]   float16 \n",
            "synthesis.b256:1      -           -        [16, 64, 256, 256]   float32 \n",
            "---                   ---         ---      ---                  ---     \n",
            "Total                 23191522    175568   -                    -       \n",
            "\n",
            "\n",
            "Discriminator  Parameters  Buffers  Output shape         Datatype\n",
            "---            ---         ---      ---                  ---     \n",
            "b256.fromrgb   256         16       [16, 64, 256, 256]   float16 \n",
            "b256.skip      8192        16       [16, 128, 128, 128]  float16 \n",
            "b256.conv0     36928       16       [16, 64, 256, 256]   float16 \n",
            "b256.conv1     73856       16       [16, 128, 128, 128]  float16 \n",
            "b256           -           16       [16, 128, 128, 128]  float16 \n",
            "b128.skip      32768       16       [16, 256, 64, 64]    float16 \n",
            "b128.conv0     147584      16       [16, 128, 128, 128]  float16 \n",
            "b128.conv1     295168      16       [16, 256, 64, 64]    float16 \n",
            "b128           -           16       [16, 256, 64, 64]    float16 \n",
            "b64.skip       131072      16       [16, 512, 32, 32]    float16 \n",
            "b64.conv0      590080      16       [16, 256, 64, 64]    float16 \n",
            "b64.conv1      1180160     16       [16, 512, 32, 32]    float16 \n",
            "b64            -           16       [16, 512, 32, 32]    float16 \n",
            "b32.skip       262144      16       [16, 512, 16, 16]    float16 \n",
            "b32.conv0      2359808     16       [16, 512, 32, 32]    float16 \n",
            "b32.conv1      2359808     16       [16, 512, 16, 16]    float16 \n",
            "b32            -           16       [16, 512, 16, 16]    float16 \n",
            "b16.skip       262144      16       [16, 512, 8, 8]      float32 \n",
            "b16.conv0      2359808     16       [16, 512, 16, 16]    float32 \n",
            "b16.conv1      2359808     16       [16, 512, 8, 8]      float32 \n",
            "b16            -           16       [16, 512, 8, 8]      float32 \n",
            "b8.skip        262144      16       [16, 512, 4, 4]      float32 \n",
            "b8.conv0       2359808     16       [16, 512, 8, 8]      float32 \n",
            "b8.conv1       2359808     16       [16, 512, 4, 4]      float32 \n",
            "b8             -           16       [16, 512, 4, 4]      float32 \n",
            "b4.mbstd       -           -        [16, 513, 4, 4]      float32 \n",
            "b4.conv        2364416     16       [16, 512, 4, 4]      float32 \n",
            "b4.fc          4194816     -        [16, 512]            float32 \n",
            "b4.out         513         -        [16, 1]              float32 \n",
            "---            ---         ---      ---                  ---     \n",
            "Total          24001089    416      -                    -       \n",
            "\n",
            "Setting up augmentation...\n",
            "Distributing across 1 GPUs...\n",
            "Setting up training phases...\n",
            "Exporting sample images...\n",
            "Initializing logs...\n",
            "Training for 25000 kimg...\n",
            "\n",
            "tick 0     kimg 0.0      time 1m 19s       sec/tick 18.5    sec/kimg 1153.47 maintenance 61.0   cpumem 3.99   gpumem 9.09   augment 0.000\n",
            "Evaluating metrics...\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 3 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1051: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return forward_call(*input, **kwargs)\n",
            "/usr/lib/python3.7/multiprocessing/semaphore_tracker.py:144: UserWarning: semaphore_tracker: There appear to be 34 leaked semaphores to clean up at shutdown\n",
            "  len(cache))\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Modify these to suit your needs\n",
        "EXPERIMENTS = \"/content/drive/MyDrive/Datasets/Kaggle/Landscape/experiments\"\n",
        "NETWORK = \"network-snapshot-000000.pkl\"\n",
        "RESUME = os.path.join(EXPERIMENTS, \"00001-Processed_mountain-auto1-resumecustom\", NETWORK)\n",
        "DATA = \"/content/drive/MyDrive/Datasets/Kaggle/Landscape/seg_train/seg_train/Processed_mountain\"\n",
        "SNAP = 10\n",
        "\n",
        "# Build the command and run it\n",
        "cmd = f\"/usr/bin/python3 /content/stylegan2-ada-pytorch/train.py --snap {SNAP} --resume {RESUME} --outdir {EXPERIMENTS} --data {DATA}\"\n",
        "!{cmd}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cxzG_5KvrMOG"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "08f2898142b044479517f525d2c82454": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "15f2d69ea54e4f809b3eb8ba056055cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ad7fb1133ad548d3a04797f3176043f3",
            "placeholder": "​",
            "style": "IPY_MODEL_b5f59378237f41cc81044e4222ed3e68",
            "value": " 2512/2512 [00:00&lt;00:00, 5435.04it/s]"
          }
        },
        "56d424255bf142bc97ffc9bfbb4b7f9a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e2e966131b54a0c927d8ff512ba3205": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7cfd0845dc414799b7e041cdb540b87b",
              "IPY_MODEL_bfe48e018c6a4ac898182163491c2a15",
              "IPY_MODEL_15f2d69ea54e4f809b3eb8ba056055cb"
            ],
            "layout": "IPY_MODEL_979d78ed5d3a4dc5a1e7fd00ed24bcea"
          }
        },
        "7556cf7ae5c744aaafb0dd64381decb8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7cfd0845dc414799b7e041cdb540b87b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_56d424255bf142bc97ffc9bfbb4b7f9a",
            "placeholder": "​",
            "style": "IPY_MODEL_7556cf7ae5c744aaafb0dd64381decb8",
            "value": "100%"
          }
        },
        "979d78ed5d3a4dc5a1e7fd00ed24bcea": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ad7fb1133ad548d3a04797f3176043f3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b5f59378237f41cc81044e4222ed3e68": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bfe48e018c6a4ac898182163491c2a15": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_08f2898142b044479517f525d2c82454",
            "max": 2512,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e88bba6ffb7342a9a35c67f2287f23d7",
            "value": 2512
          }
        },
        "e88bba6ffb7342a9a35c67f2287f23d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}